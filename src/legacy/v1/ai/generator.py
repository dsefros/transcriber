"""
–ï–¥–∏–Ω—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –±—ç–∫–µ–Ω–¥–æ–≤
"""
import ollama
import torch
import gc
from pathlib import Path
from src.legacy.config.models import ModelProfile

# –ö—ç—à –¥–ª—è –ª–µ–Ω–∏–≤–æ–π –∑–∞–≥—Ä—É–∑–∫–∏
_OLLAMA_CLIENT = ollama.Client()
_LLAMA_CPP_MODEL = None
_LLAMA_CPP_PROFILE_KEY = None

def _free_gpu_memory():
    """–ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–µ –æ—Å–≤–æ–±–æ–∂–¥–µ–Ω–∏–µ –ø–∞–º—è—Ç–∏"""
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.synchronize()
        gc.collect()

def _generate_ollama(prompt: str, profile: ModelProfile) -> str:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —á–µ—Ä–µ–∑ Ollama API"""
    generation_options = {
        "temperature": profile.params.get("temperature", 0.1),
        "top_p": profile.params.get("top_p", 0.9),
        "repeat_penalty": profile.params.get("repeat_penalty", 1.15),
        "num_predict": profile.params.get("num_predict", 2000),
    }
    generation_options = {k: v for k, v in generation_options.items() if v is not None}
    
    try:
        response = _OLLAMA_CLIENT.chat(
            model=profile.name,
            messages=[{"role": "user", "content": prompt}],
            options=generation_options,
            stream=False
        )
        return response["message"]["content"].strip()
    except Exception as e:
        raise RuntimeError(f"Ollama –æ—à–∏–±–∫–∞ ({profile.name}): {e}")

def _load_llama_cpp_model(profile: ModelProfile):
    """–õ–µ–Ω–∏–≤–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ llama.cpp —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º –∏ –æ—á–∏—Å—Ç–∫–æ–π –ø–∞–º—è—Ç–∏"""
    global _LLAMA_CPP_MODEL, _LLAMA_CPP_PROFILE_KEY
    if _LLAMA_CPP_MODEL is not None and _LLAMA_CPP_PROFILE_KEY == profile.key:
        return _LLAMA_CPP_MODEL
    
    # üî• –û—á–∏—Å—Ç–∫–∞ –ø–µ—Ä–µ–¥ –∑–∞–≥—Ä—É–∑–∫–æ–π –Ω–æ–≤–æ–π –º–æ–¥–µ–ª–∏
    _free_gpu_memory()
    
    model_path = Path(profile.path).resolve()
    if not model_path.exists():
        raise FileNotFoundError(f"–§–∞–π–ª –º–æ–¥–µ–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω: {model_path}")
    
    from llama_cpp import Llama
    load_params = {
        "model_path": str(model_path),
        "n_ctx": profile.params.get("n_ctx", 4096),
        "n_gpu_layers": profile.params.get("n_gpu_layers", 35),
        "n_batch": profile.params.get("n_batch", 512),
        "verbose": profile.params.get("verbose", False),
    }
    print(f"‚è≥ –ó–∞–≥—Ä—É–∑–∫–∞ {profile.key} –≤ GPU (—Å–ª–æ—ë–≤: {load_params['n_gpu_layers']})...")
    _LLAMA_CPP_MODEL = Llama(**load_params)
    _LLAMA_CPP_PROFILE_KEY = profile.key
    print(f"‚úÖ {profile.key} –≥–æ—Ç–æ–≤–∞")
    return _LLAMA_CPP_MODEL

def _format_prompt_for_model(prompt: str, model_key: str) -> str:
    """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–∞ –ø–æ–¥ —Å–ø–µ—Ü–∏—Ñ–∏–∫—É –º–æ–¥–µ–ª–∏ (–±–µ–∑ –¥—É–±–ª–∏—Ä—É—é—â–µ–≥–æ <s>)"""
    if "mistral" in model_key.lower():
        return f"[INST] {prompt} [/INST]"
    elif "phi3" in model_key.lower():
        return f"<|user|>\n{prompt}<|end|>\n<|assistant|>\n"
    else:
        return prompt

def _generate_llama_cpp(prompt: str, profile: ModelProfile) -> str:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —á–µ—Ä–µ–∑ llama-cpp-python"""
    try:
        llm = _load_llama_cpp_model(profile)
        formatted_prompt = _format_prompt_for_model(prompt, profile.key)
        generation_params = {
            "prompt": formatted_prompt,
            "temperature": profile.params.get("temperature", 0.1),
            "top_p": profile.params.get("top_p", 0.9),
            "repeat_penalty": profile.params.get("repeat_penalty", 1.15),
            "max_tokens": profile.params.get("max_tokens", 2048),
            "stop": ["</s>", "<|end|>", "<|user|>", "<|assistant|>", "[INST]", "[/INST]"],
        }
        generation_params = {k: v for k, v in generation_params.items() if v is not None}
        response = llm(**generation_params)
        text = response["choices"][0]["text"].strip()
        for token in ["</s>", "<|end|>", "<|user|>", "<|assistant|>", "[INST]", "[/INST]"]:
            text = text.replace(token, "").strip()
        return text
    except Exception as e:
        raise RuntimeError(f"llama-cpp –æ—à–∏–±–∫–∞ ({profile.key}): {e}")

def generate_text(prompt: str, profile: ModelProfile) -> str:
    """–ï–¥–∏–Ω–∞—è —Ç–æ—á–∫–∞ –≤—Ö–æ–¥–∞ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏"""
    if profile.backend == "ollama":
        return _generate_ollama(prompt, profile)
    elif profile.backend == "llama_cpp":
        return _generate_llama_cpp(prompt, profile)
    else:
        raise ValueError(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –±—ç–∫–µ–Ω–¥: {profile.backend}")