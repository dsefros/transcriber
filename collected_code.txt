***
collect_py.py
***
import os

def collect_python_files(root_dir: str = ".", output_file: str = "collected_code.txt", exclude_dirs: set = None):
    if exclude_dirs is None:
        exclude_dirs = {"venv", "__pycache__"}

    with open(output_file, "w", encoding="utf-8") as out_f:
        for dirpath, dirnames, filenames in os.walk(root_dir):
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∏—Å–∫–ª—é—á—ë–Ω–Ω—ã–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
            dirnames[:] = [d for d in dirnames if d not in exclude_dirs]
            for filename in sorted(filenames):
                if filename.endswith(".py"):
                    filepath = os.path.join(dirpath, filename)
                    rel_path = os.path.relpath(filepath, root_dir)
                    try:
                        with open(filepath, "r", encoding="utf-8") as f:
                            content = f.read()
                        out_f.write(f"***\n{rel_path}\n***\n{content}\n***\n")
                    except Exception as e:
                        out_f.write(f"***\n{rel_path}\n***\n<–û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ —Ñ–∞–π–ª–∞: {e}>\n***\n")

if __name__ == "__main__":
    collect_python_files()
***
***
src/cli.py
***
#!/usr/bin/env python3
"""CLI –¥–ª—è pipeline –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤—Å—Ç—Ä–µ—á."""
import argparse
from pathlib import Path
from src.pipeline.main import main as run_pipeline

def main():
    parser = argparse.ArgumentParser(description="Pipeline –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –≤—Å—Ç—Ä–µ—á")
    parser.add_argument("audio_file", type=Path, help="–ü—É—Ç—å –∫ –∞—É–¥–∏–æ—Ñ–∞–π–ª—É")
    parser.add_argument("--device", choices=["cuda", "cpu"], default="cuda")
    args = parser.parse_args()
    
    # –í–∞–ª–∏–¥–∞—Ü–∏—è —Ñ–∞–π–ª–∞
    if not args.audio_file.exists():
        raise FileNotFoundError(f"–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {args.audio_file}")
    
    # –ó–∞–ø—É—Å–∫ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –ø–∞–π–ø–ª–∞–π–Ω–∞
    run_pipeline(str(args.audio_file), args.device)

if __name__ == "__main__":
    main()

***
***
src/pipeline/main.py
***
#!/usr/bin/env python3
"""
–ï–¥–∏–Ω—ã–π pipeline –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤—Å—Ç—Ä–µ—á:
1. –¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—è + –¥–∏–∞—Ä–∏–∑–∞—Ü–∏—è —á–µ—Ä–µ–∑ WhisperX
2. –ê–Ω–∞–ª–∏–∑ —á–µ—Ä–µ–∑ –ª–æ–∫–∞–ª—å–Ω—É—é LLM (Ollama)
3. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –æ—Ç—á—ë—Ç–∞ –≤ Markdown
4. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –≤ Postgres –∏ Qdrant

–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç offline-—Ä–µ–∂–∏–º –±–µ–∑ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–∞.
"""
import os
import sys
import json
import argparse
from pathlib import Path
from dotenv import load_dotenv
import torch
import ollama
from datetime import datetime
import hashlib
import traceback

# === –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ ===
load_dotenv()
INPUT_DIR = os.getenv("INPUT_DIR", "input")
OUTPUT_DIR = os.getenv("OUTPUT_DIR", "output")
MODEL_NAME = os.getenv("MODEL_NAME", "phi3:medium-128k")
HF_TOKEN = os.getenv("HF_TOKEN")

os.makedirs(OUTPUT_DIR, exist_ok=True)

# === –ò–º–ø–æ—Ä—Ç –º–æ–¥—É–ª–µ–π –±–∞–∑ –¥–∞–Ω–Ω—ã—Ö ===
from src.storage.postgres import init_db, get_db_session, Meeting, Speaker, Fragment
from src.storage.qdrant import init_qdrant_client, create_collections_if_not_exists
import hashlib
import json

# === 1. –¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—è –∏ –¥–∏–∞—Ä–∏–∑–∞—Ü–∏—è (–Ω–∞ –æ—Å–Ω–æ–≤–µ transcribe_v3.py) ===
def transcribe_and_diarize(audio_path: str, device: str = "cuda"):
    """–¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—è + –¥–∏–∞—Ä–∏–∑–∞—Ü–∏—è —á–µ—Ä–µ–∑ WhisperX."""
    import whisperx
    from pydub import AudioSegment
    
    print(f"[DEBUG] –ó–∞–≥—Ä—É–∑–∫–∞ WhisperX (large-v3) –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ: {device}")
    model = whisperx.load_model("large-v3", device, compute_type="float16" if device == "cuda" else "int8")
    
    # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ WAV 16kHz –º–æ–Ω–æ
    print("[DEBUG] –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –∞—É–¥–∏–æ –≤ WAV...")
    audio = AudioSegment.from_file(audio_path)
    audio = audio.set_channels(1).set_frame_rate(16000)
    wav_path = "temp_audio.wav"
    audio.export(wav_path, format="wav")
    
    # –¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—è
    print("[DEBUG] –¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—è...")
    audio_data = whisperx.load_audio(wav_path)
    result = model.transcribe(audio_data, language="ru")
    
    # –í—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ
    model_a, metadata = whisperx.load_align_model(
        language_code="ru",
        device=device,
        model_name="facebook/wav2vec2-base-960h"
    )
    
    # –î–∏–∞—Ä–∏–∑–∞—Ü–∏—è
    print("[DEBUG] –î–∏–∞—Ä–∏–∑–∞—Ü–∏—è...")
    diarize_model = whisperx.DiarizationPipeline(use_auth_token=HF_TOKEN, device=device)
    diarize_segments = diarize_model(audio_data)
    result = whisperx.assign_word_speakers(diarize_segments, result)
    
    # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–µ–≥–º–µ–Ω—Ç–æ–≤
    segments = []
    for seg in result["segments"]:
        segments.append({
            "text": seg.get("text", "").strip(),
            "start": round(seg.get("start", 0), 2),
            "end": round(seg.get("end", 0), 2),
            "speaker": seg.get("speaker", "SPEAKER_00")
        })
    
    # –û—á–∏—Å—Ç–∫–∞ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
    if os.path.exists(wav_path):
        os.remove(wav_path)
    
    print(f"[DEBUG] WhisperX –∑–∞–≤–µ—Ä—à—ë–Ω. –°–µ–≥–º–µ–Ω—Ç–æ–≤: {len(segments)}")
    return segments

# === 2. –ê–Ω–∞–ª–∏–∑ —á–µ—Ä–µ–∑ Ollama (–ª–æ–∫–∞–ª—å–Ω–∞—è LLM) ===
def analyze_with_ollama(segments: list) -> str:
    """–ê–Ω–∞–ª–∏–∑ –¥–∏–∞–ª–æ–≥–∞ —á–µ—Ä–µ–∑ –ª–æ–∫–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å Ollama."""
    print(f"[DEBUG] –ê–Ω–∞–ª–∏–∑ —á–µ—Ä–µ–∑ Ollama ({MODEL_NAME})...")
    
    # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∏–∞–ª–æ–≥–∞ –¥–ª—è –ø—Ä–æ–º–ø—Ç–∞
    dialogue = "\n".join([f"{seg['speaker']}: {seg['text']}" for seg in segments if seg["text"]])
    
    # –°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç, –∑–∞—Ç–æ—á–µ–Ω–Ω—ã–π –ø–æ–¥ –≤–∞—à—É –ø—Ä–µ–¥–º–µ—Ç–Ω—É—é –æ–±–ª–∞—Å—Ç—å
    system_prompt = """
–¢—ã ‚Äî —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∞–Ω–∞–ª–∏–∑—É —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –≤—Å—Ç—Ä–µ—á –≤ –∫–æ–º–ø–∞–Ω–∏–∏ –°–æ–º–º–µ—Ä—Å. –°–æ–º–º–µ—Ä—Å —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç—Å—è –Ω–∞ IT-—Ä–µ—à–µ–Ω–∏—è—Ö –¥–ª—è —ç–∫–≤–∞–π—Ä–∏–Ω–≥–∞, POS-—Ç–µ—Ä–º–∏–Ω–∞–ª–æ–≤ –∏ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –±–∏–∑–Ω–µ—Å–∞. –¢–≤–æ–∏ –∫–ª–∏–µ–Ω—Ç—ã ‚Äî –±–∞–Ω–∫–∏, —Ä–∏—Ç–µ–π–ª –∏ —á–∞—Å—Ç–Ω—ã–µ –º–µ—Ä—á–∞–Ω—Ç—ã.

–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –¥–∏–∞–ª–æ–≥ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤—å –°–¢–†–£–ö–¢–£–†–ò–†–û–í–ê–ù–ù–´–ô –û–¢–ß–Å–¢ –≤ —Ñ–æ—Ä–º–∞—Ç–µ Markdown —Å–æ —Å–ª–µ–¥—É—é—â–∏–º–∏ —Ä–∞–∑–¥–µ–ª–∞–º–∏:

### üìù –ö—Ä–∞—Ç–∫–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ
- –û–±—â–∞—è —Ç–µ–º–∞ –≤—Å—Ç—Ä–µ—á–∏ (1-2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è)

### ‚ö†Ô∏è –ü—Ä–æ–±–ª–µ–º—ã –∏ —Ä–µ—à–µ–Ω–∏—è
–î–ª—è –∫–∞–∂–¥–æ–π —Ç–µ–º—ã –æ–±—Å—É–∂–¥–µ–Ω–∏—è:
- –ù–∞–∑–≤–∞–Ω–∏–µ –ø—Ä–æ–±–ª–µ–º—ã (–∫—Ä–∞—Ç–∫–æ)
- –ö—Ç–æ –æ–∑–≤—É—á–∏–ª –ø—Ä–æ–±–ª–µ–º—É (—Å–ø–∏–∫–µ—Ä)
- –°—É—Ç—å –ø—Ä–æ–±–ª–µ–º—ã
- –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–µ —Ä–µ—à–µ–Ω–∏—è (–µ—Å–ª–∏ –µ—Å—Ç—å)
- –ö—Ç–æ –ø—Ä–µ–¥–ª–æ–∂–∏–ª —Ä–µ—à–µ–Ω–∏–µ (—Å–ø–∏–∫–µ—Ä)
- –°—Ç–∞—Ç—É—Å —Ä–µ—à–µ–Ω–∏—è (—Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–æ/–≤ —Ä–∞–±–æ—Ç–µ/—Ç—Ä–µ–±—É–µ—Ç —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–∏—è)

–í–ê–ñ–ù–û:
1. –ï—Å–ª–∏ –≤ –¥–∏–∞–ª–æ–≥–µ –µ—Å—Ç—å —É–ø–æ–º–∏–Ω–∞–Ω–∏—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö –¥–∞—Ç ‚Äî –∏—Å–ø–æ–ª—å–∑—É–π –∏—Ö.
2. –ï—Å–ª–∏ –¥–∞—Ç—ã –Ω–µ—Ç, –Ω–æ –µ—Å—Ç—å —É–∫–∞–∑–∞–Ω–∏–µ –Ω–∞ —Å—Ä–æ–∫ ("—Å–µ–≥–æ–¥–Ω—è", "–Ω–∞ —Å–ª–µ–¥—É—é—â–µ–π –Ω–µ–¥–µ–ª–µ") ‚Äî –ø—Ä–µ–æ–±—Ä–∞–∑—É–π –≤ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—É—é –¥–∞—Ç—É –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ —Ç–µ–∫—É—â–µ–π –¥–∞—Ç—ã {datetime.now().strftime('%d.%m.%Y')}.
3. –§–æ–∫—É—Å–∏—Ä—É–π—Å—è –Ω–∞ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –¥–µ—Ç–∞–ª—è—Ö: –°–ë–ü, —Ç–µ—Ä–º–∏–Ω–∞–ª—ã (Ingenico/Newland), –ø—Ä–æ—à–∏–≤–∫–∏, –≤–æ–∑–≤—Ä–∞—Ç—ã, —Å—Ç–µ–π—Ç-—Ö–æ–ª–¥–µ—Ä—ã, –ª–æ–≥–∏.
4. –ù–µ –ø—Ä–∏–¥—É–º—ã–≤–∞–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é, –∫–æ—Ç–æ—Ä–æ–π –Ω–µ—Ç –≤ –¥–∏–∞–ª–æ–≥–µ.
5. –ï—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –Ω–µ–ø–æ–ª–Ω–∞—è ‚Äî –ø–æ–º–µ—á–∞–π —Å—Ç–∞—Ç—É—Å –∫–∞–∫ "–¢—Ä–µ–±—É–µ—Ç —É—Ç–æ—á–Ω–µ–Ω–∏—è".

–û—Ç–≤–µ—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Å—Ç—Ä–æ–≥–æ –≤ —Ñ–æ—Ä–º–∞—Ç–µ Markdown –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤.
"""

    # –ó–∞–ø—Ä–æ—Å –∫ Ollama
    try:
        response = ollama.chat(
            model=MODEL_NAME,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": f"–î–∏–∞–ª–æ–≥ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞:\n{dialogue}"}
            ],
            options={
                "temperature": float(os.getenv("TEMPERATURE", 0.1)),
                "top_p": float(os.getenv("TOP_P", 0.9)),
                "repeat_penalty": float(os.getenv("REPEAT_PENALTY", 1.15)),
                "num_predict": 2000
            }
        )
        return response["message"]["content"]
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ –∫ Ollama: {e}")
        # –†–µ–∑–µ—Ä–≤–Ω—ã–π –∞–Ω–∞–ª–∏–∑ (—É–ø—Ä–æ—â—ë–Ω–Ω—ã–π)
        return f"‚ö†Ô∏è **–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞**\n\n–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –¥–∏–∞–ª–æ–≥ –∏–∑-–∑–∞ –æ—à–∏–±–∫–∏:\n`{str(e)}`\n\n**–°—ã—Ä—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–∏–∞–ª–æ–≥–∞:**\n{dialogue[:1000]}..."

# === 3. –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤ ===
def extract_technical_terms(text: str) -> list:
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤ –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
    technical_terms = [
        'TSP', '–ü–ò–æ–¢', '–ï–ì–ê–ò–°', '–º–∞—Ä–∫–∏—Ä–æ–≤–∫–∞', '–æ—Ñ–ª–∞–π–Ω', '–ß–µ—Å—Ç–Ω—ã–π –ó–ù–ê–ö',
        '–°–ë–ü', '—ç–∫–≤–∞–π—Ä–∏–Ω–≥', 'POS', '—Ç–µ—Ä–º–∏–Ω–∞–ª', '–ø—Ä–æ—à–∏–≤–∫–∞', '–≤–æ–∑–≤—Ä–∞—Ç—ã',
        '—Å—Ç–µ–π—Ç-—Ö–æ–ª–¥–µ—Ä—ã', '–ª–æ–≥–∏', 'API', '–∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è', '–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å',
        'Sommers', '–°–æ–º–º–µ—Ä—Å', '–∫–æ–Ω—Ç—Ä–∞–≥–µ–Ω—Ç', '–∫–ª–∏–µ–Ω—Ç', '–ø—Ä–æ–¥—É–∫—Ç', '—Ç–∞—Ä–∏—Ñ',
        'UTM', '—Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏—è', '–∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è', '–æ—Ç–∫–∞–∑', '–∫–æ–º–∏—Å—Å–∏—è'
    ]
    
    found_terms = []
    text_lower = text.lower()
    
    for term in technical_terms:
        if term.lower() in text_lower:
            found_terms.append(term)
    
    return found_terms

# === 4. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö ===
def save_to_databases(session, qdrant_client, filename, segments, analysis_md, original_audio_path):
    """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ Postgres –∏ Qdrant"""
    print("üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –≤ –±–∞–∑—ã...")
    
    try:
        # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Ö–µ—à–∞ –∞—É–¥–∏–æ—Ñ–∞–π–ª–∞
        with open(original_audio_path, 'rb') as f:
            audio_content = f.read()
            audio_hash = hashlib.sha256(audio_content).hexdigest()
        
        # 1. –°–æ–∑–¥–∞–Ω–∏–µ –∑–∞–ø–∏—Å–∏ –æ –≤—Å—Ç—Ä–µ—á–µ
        meeting = Meeting(
            filename=filename,
            start_time=datetime.now(),
            duration_sec=sum(seg.get('end', 0) - seg.get('start', 0) for seg in segments),
            audio_hash=audio_hash,
            status='completed',
            quality_score=0.95,
            context_tags=json.dumps(["meeting", "transcription"])
        )
        session.add(meeting)
        session.flush()
        
        # 2. –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–ø–∏–∫–µ—Ä–æ–≤
        speaker_cache = {}
        
        for seg in segments:
            speaker_name = seg.get('speaker', 'SPEAKER_00')
            
            if speaker_name not in speaker_cache:
                # –ü–æ–∏—Å–∫ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ —Å–ø–∏–∫–µ—Ä–∞
                speaker = session.query(Speaker).filter_by(external_id=speaker_name).first()
                if not speaker:
                    # –°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤–æ–≥–æ —Å–ø–∏–∫–µ—Ä–∞
                    speaker = Speaker(
                        external_id=speaker_name,
                        name=speaker_name,
                        role='–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ'
                    )
                    session.add(speaker)
                    session.flush()
                speaker_cache[speaker_name] = speaker.id
        
        # 3. –°–æ–∑–¥–∞–Ω–∏–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤
        for i, seg in enumerate(segments):
            speaker_name = seg.get('speaker', 'SPEAKER_00')
            speaker_id = speaker_cache[speaker_name]
            
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤
            technical_terms = extract_technical_terms(seg.get('text', ''))
            
            fragment = Fragment(
                meeting_id=meeting.id,
                start_time=seg.get('start', 0),
                end_time=seg.get('end', 0),
                speaker_id=speaker_id,
                text=seg.get('text', '').strip(),
                raw_text=seg.get('text', ''),
                importance_score=0.8,
                business_value='–æ–±—Å—É–∂–¥–µ–Ω–∏–µ',
                technical_terms=json.dumps(technical_terms),
                semantic_cluster=i // 5  # –ü—Ä–æ—Å—Ç–∞—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –ø–æ 5 —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤
            )
            session.add(fragment)
            session.flush()
        
        # 4. –ö–æ–º–º–∏—Ç –∏–∑–º–µ–Ω–µ–Ω–∏–π
        session.commit()
        print(f"‚úÖ –î–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ Postgres! –í—Å—Ç—Ä–µ—á–∞ ID: {meeting.id}")
        
        # 5. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç—á—ë—Ç–∞ –≤ —Ñ–∞–π–ª (–¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏)
        base_name = Path(filename).stem
        md_path = os.path.join(OUTPUT_DIR, f"{base_name}.md")
        with open(md_path, "w", encoding="utf-8") as f:
            f.write(analysis_md)
        
        return md_path
        
    except Exception as e:
        session.rollback()
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –≤ –±–∞–∑—ã: {e}")
        traceback.print_exc()
        raise
    finally:
        session.close()

# === –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è ===
def main(audio_file: str, device: str = "cuda"):
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏—è —Ñ–∞–π–ª–∞
    if not os.path.exists(audio_file):
        print(f"‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {audio_file}")
        sys.exit(1)

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞
    if device == "cuda" and not torch.cuda.is_available():
        print("‚ö†Ô∏è CUDA –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞, –ø–µ—Ä–µ–∫–ª—é—á–∞—é—Å—å –Ω–∞ CPU")
        device = "cpu"

    print(f"\nüöÄ –ó–∞–ø—É—Å–∫ pipeline –¥–ª—è —Ñ–∞–π–ª–∞: {audio_file}")
    print(f"‚öôÔ∏è –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}")
    print(f"üß† –ú–æ–¥–µ–ª—å –∞–Ω–∞–ª–∏–∑–∞: {MODEL_NAME}\n")

    try:
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–∞–∑ –¥–∞–Ω–Ω—ã—Ö
        print("üîß –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–∞–∑ –¥–∞–Ω–Ω—ã—Ö...")
        engine = init_db()
        qdrant_client = init_qdrant_client()
        create_collections_if_not_exists(qdrant_client)

        session = get_db_session(engine)

        # 1. –¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—è –∏ –¥–∏–∞—Ä–∏–∑–∞—Ü–∏—è
        segments = transcribe_and_diarize(audio_file, device=device)

        if not segments:
            print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Å–µ–≥–º–µ–Ω—Ç—ã —Ä–µ—á–∏. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –∞—É–¥–∏–æ—Ñ–∞–π–ª.")
            sys.exit(1)

        print(f"‚úÖ –ü–æ–ª—É—á–µ–Ω–æ {len(segments)} —Å–µ–≥–º–µ–Ω—Ç–æ–≤")

        # 2. –ê–Ω–∞–ª–∏–∑ —á–µ—Ä–µ–∑ Ollama
        analysis_md = analyze_with_ollama(segments)

        # 3. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
        md_path = save_to_databases(session, qdrant_client, os.path.basename(audio_file), segments, analysis_md, audio_file)

        print(f"\nüéâ –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à—ë–Ω! –ü–æ–ª–Ω—ã–π –æ—Ç—á—ë—Ç:\n{md_path}")

        # –í—ã–≤–æ–¥ –∫—Ä–∞—Ç–∫–æ–≥–æ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è –≤ –∫–æ–Ω—Å–æ–ª—å
        summary_start = analysis_md.find("### üìù –ö—Ä–∞—Ç–∫–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ")
        if summary_start != -1:
            summary_end = analysis_md.find("###", summary_start + 1)
            if summary_end == -1:
                summary_end = len(analysis_md)
            print("\nüìã –ö–†–ê–¢–ö–û–ï –°–û–î–ï–†–ñ–ê–ù–ò–ï:")
            print(analysis_md[summary_start:summary_end].strip())

    except KeyboardInterrupt:
        print("\nüõë –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–µ—Ä–≤–∞–Ω–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
        sys.exit(1)
    except Exception as e:
        print(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
        traceback.print_exc()
        sys.exit(1)
***
***
src/utils/term_loader.py
***
from qdrant_client import QdrantClient
from qdrant_client.http import models
from sentence_transformers import SentenceTransformer
import uuid

client = QdrantClient(host='localhost', port=6333)
encoder = SentenceTransformer('all-MiniLM-L6-v2')

term_data = {
    'term_id': 'tms',
    'term_name': 'TMS',
    'full_name': 'Terminal Management System',
    'definition': '–ü—Ä–æ–≥—Ä–∞–º–º–Ω–æ–µ –æ–±–µ—Å–ø–µ—á–µ–Ω–∏–µ –¥–ª—è —Ü–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–≥–æ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ø–∞—Ä–∫–æ–º POS-—Ç–µ—Ä–º–∏–Ω–∞–ª–æ–≤. –û–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç —É–¥–∞–ª–µ–Ω–Ω—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é, –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ —Å–æ—Å—Ç–æ—è–Ω–∏—è, –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø—Ä–æ—à–∏–≤–æ–∫ –∏ —Å–±–æ—Ä –¥–∏–∞–≥–Ω–æ—Å—Ç–∏—á–µ—Å–∫–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ —Å —Ç–µ—Ä–º–∏–Ω–∞–ª–æ–≤.',
    'related_terms': ['–ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö', 'POS-—Ç–µ—Ä–º–∏–Ω–∞–ª', '–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è', '–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥', '–õ–∏—Ü–µ–Ω–∑–∏—Ä–æ–≤–∞–Ω–∏–µ', 'Somers.POS'],
    'business_context': ['—É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Å–µ—Ç—å—é —Ç–µ—Ä–º–∏–Ω–∞–ª–æ–≤', '–†–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏–µ –Ω–æ–≤—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π –Ω–∞ —Ç–µ—Ä–º–∏–Ω–∞–ª–∞—Ö', '–î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –ø—Ä–æ–±–ª–µ–º —Å —Ç–µ—Ä–º–∏–Ω–∞–ª–∞–º–∏', '–û–±–µ—Å–ø–µ—á–µ–Ω–∏–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º –ù–°–ü–ö'],
    'regulatory_info': '–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º —Ä–µ–≥—É–ª—è—Ç–æ—Ä–æ–≤ –ø–æ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø–ª–∞—Ç–µ–∂–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏ —É–¥–∞–ª–µ–Ω–Ω–æ–º—É —É–ø—Ä–∞–≤–ª–µ–Ω–∏—é —Ç–µ—Ä–º–∏–Ω–∞–ª–∞–º–∏.',
    'common_misconceptions': [],
    'examples': [
        '–¢–µ—Ä–º–∏–Ω–∞–ª —Å—Ö–æ–¥–∏–ª –Ω–∞ TMS –∏ –ø–æ–ª—É—á–∏–ª –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é.',
        '–¢–µ—Ä–º–∏–Ω–∞–ª –∏—Å–ø–æ–ª—å–∑—É–µ—Ç TMS, —á—Ç–æ-–±—ã –æ–±–Ω–æ–≤–ª—è—Ç—å —Å–≤–æ–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã.',
        '–ù–∞ TMS –º–æ–∂–Ω–æ –æ—Ç—Å–ª–µ–∂–∏–≤–∞—Ç—å —Ä–∞–±–æ—Ç—É —Ç–µ—Ä–º–∏–Ω–∞–ª–æ–≤.',
        '–ò–Ω–∂–µ–Ω–µ—Ä –°–æ–º–µ—Ä—Å –∑–∞—Ö–æ–¥–∏—Ç –≤ TMS –¥–ª—è –≤—ã—Å—Ç–∞–≤–ª–µ–Ω–∏—è –Ω–∞—Å—Ç—Ä–æ–µ–∫ –°–ë–ü –Ω–∞ –Ω–æ–≤—ã–µ —Ç–µ—Ä–º–∏–Ω–∞–ª—ã'
    ],
    'importance_level': 9,
    'last_updated': '2026-01-14'
}

embedding = encoder.encode(term_data['definition']).tolist()
# –ò—Å–ø–æ–ª—å–∑—É–µ–º UUID –≤–º–µ—Å—Ç–æ —Å—Ç—Ä–æ–∫–∏
point_id = str(uuid.uuid4())

client.upsert(
    collection_name='technical_terms',
    points=[models.PointStruct(id=point_id, vector=embedding, payload=term_data)]
)
print(f'‚úÖ TMS –¥–æ–±–∞–≤–ª–µ–Ω –≤ Qdrant! ID: {point_id}')

***
***
src/storage/postgres.py
***
import os
from sqlalchemy import create_engine, Column, Integer, String, Float, DateTime, Boolean, Text, JSON, ForeignKey, func
from sqlalchemy import text
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker, relationship
from sqlalchemy.dialects.postgresql import UUID
from datetime import datetime, timezone
import uuid
from dotenv import load_dotenv

load_dotenv()

Base = declarative_base()

class Meeting(Base):
    __tablename__ = 'meetings'
    
    id = Column(Integer, primary_key=True)
    filename = Column(String, unique=True, nullable=False)
    start_time = Column(DateTime(timezone=True), nullable=False, default=func.now())
    duration_sec = Column(Integer, nullable=False)
    audio_hash = Column(String(64), unique=True, nullable=False)
    processed_at = Column(DateTime(timezone=True), default=func.now())
    status = Column(String(20), default='raw')
    quality_score = Column(Float)
    context_tags = Column(JSON, default=[])
    created_at = Column(DateTime(timezone=True), default=func.now())
    updated_at = Column(DateTime(timezone=True), default=func.now(), onupdate=func.now())

class Speaker(Base):
    __tablename__ = 'speakers'
    
    id = Column(Integer, primary_key=True)
    external_id = Column(String(50), unique=True)
    name = Column(String(100))
    role = Column(String(50))
    voice_embedding = Column(JSON)
    confidence_score = Column(Float)
    last_confirmed = Column(DateTime(timezone=True))
    is_verified = Column(Boolean, default=False)
    voice_samples_count = Column(Integer, default=0)
    created_at = Column(DateTime(timezone=True), default=func.now())
    updated_at = Column(DateTime(timezone=True), default=func.now(), onupdate=func.now())

class Fragment(Base):
    __tablename__ = 'fragments'
    
    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    meeting_id = Column(Integer, ForeignKey('meetings.id', ondelete='CASCADE'), nullable=False)
    start_time = Column(Float, nullable=False)
    end_time = Column(Float, nullable=False)
    speaker_id = Column(Integer, ForeignKey('speakers.id'), nullable=False)
    text = Column(Text, nullable=False)
    raw_text = Column(Text, nullable=False)
    semantic_cluster = Column(Integer)
    importance_score = Column(Float, default=0.5)
    business_value = Column(String(50))
    technical_terms = Column(JSON, default=[])
    qdrant_id = Column(String(36))
    is_edited = Column(Boolean, default=False)
    created_at = Column(DateTime(timezone=True), default=func.now())
    updated_at = Column(DateTime(timezone=True), default=func.now(), onupdate=func.now())
    
    meeting = relationship("Meeting")
    speaker = relationship("Speaker")

def init_db():
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö"""
    try:
        database_url = os.getenv('DATABASE_URL')
        if not database_url:
            raise ValueError("DATABASE_URL –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –≤ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è")
        
        engine = create_engine(
            database_url,
            pool_size=5,
            max_overflow=10,
            pool_timeout=30,
            pool_recycle=1800,
            connect_args={
                "keepalives": 1,
                "keepalives_idle": 30,
                "keepalives_interval": 10,
                "keepalives_count": 5,
            }
        )
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –≤—Å–µ—Ö —Ç–∞–±–ª–∏—Ü
        Base.metadata.create_all(engine)
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è
        with engine.connect() as connection:
            version = connection.execute(text("SELECT version()")).scalar()
            print(f"‚úÖ Postgres –ø–æ–¥–∫–ª—é—á–µ–Ω —É—Å–ø–µ—à–Ω–æ! –í–µ—Ä—Å–∏—è: {version}")
        
        return engine
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö: {e}")
        raise

def get_db_session(engine=None):
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–µ—Å—Å–∏–∏ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö"""
    if engine is None:
        engine = init_db()
    
    Session = sessionmaker(bind=engine)
    return Session()

***
***
src/storage/qdrant.py
***
import os
from dotenv import load_dotenv
from qdrant_client import QdrantClient
from qdrant_client.http.models import Distance, VectorParams

load_dotenv()

def init_qdrant_client():
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Qdrant –∫–ª–∏–µ–Ω—Ç–∞"""
    try:
        host = os.getenv('QDRANT_HOST', 'localhost')
        port = int(os.getenv('QDRANT_PORT', '6333'))
        api_key = os.getenv('QDRANT_API_KEY', None)
        
        # –û—Ç–∫–ª—é—á–µ–Ω–∏–µ HTTPS –¥–ª—è –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è
        client = QdrantClient(
            host=host,
            port=port,
            api_key=api_key,
            https=False,
            prefer_grpc=False
        )
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è
        collections = client.get_collections()
        collection_names = [col.name for col in collections.collections]
        print(f"‚úÖ Qdrant –ø–æ–¥–∫–ª—é—á–µ–Ω —É—Å–ø–µ—à–Ω–æ! –°—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –∫–æ–ª–ª–µ–∫—Ü–∏–∏: {collection_names}")
        
        return client
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ Qdrant: {e}")
        raise

def create_collections_if_not_exists(client):
    """–°–æ–∑–¥–∞–Ω–∏–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –∫–æ–ª–ª–µ–∫—Ü–∏–π –µ—Å–ª–∏ –æ–Ω–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É—é—Ç"""
    try:
        # –ö–æ–ª–ª–µ–∫—Ü–∏—è –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤
        if not any(col.name == "semantic_fragments" for col in client.get_collections().collections):
            client.create_collection(
                collection_name="semantic_fragments",
                vectors_config=VectorParams(
                    size=384,  # –†–∞–∑–º–µ—Ä —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ all-MiniLM-L6-v2
                    distance=Distance.COSINE
                )
            )
            print("‚úÖ –ö–æ–ª–ª–µ–∫—Ü–∏—è 'semantic_fragments' —Å–æ–∑–¥–∞–Ω–∞ –≤ Qdrant")
        
        # –ö–æ–ª–ª–µ–∫—Ü–∏—è –¥–ª—è —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤
        if not any(col.name == "technical_terms" for col in client.get_collections().collections):
            client.create_collection(
                collection_name="technical_terms",
                vectors_config=VectorParams(
                    size=384,
                    distance=Distance.COSINE
                )
            )
            print("‚úÖ –ö–æ–ª–ª–µ–∫—Ü–∏—è 'technical_terms' —Å–æ–∑–¥–∞–Ω–∞ –≤ Qdrant")
        
        # –ö–æ–ª–ª–µ–∫—Ü–∏—è –¥–ª—è –±–∏–∑–Ω–µ—Å-–∫–æ–Ω—Ü–µ–ø—Ü–∏–π
        if not any(col.name == "business_concepts" for col in client.get_collections().collections):
            client.create_collection(
                collection_name="business_concepts",
                vectors_config=VectorParams(
                    size=384,
                    distance=Distance.COSINE
                )
            )
            print("‚úÖ –ö–æ–ª–ª–µ–∫—Ü–∏—è 'business_concepts' —Å–æ–∑–¥–∞–Ω–∞ –≤ Qdrant")
            
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –∫–æ–ª–ª–µ–∫—Ü–∏–π –≤ Qdrant: {e}")
        raise

***
