***
collect_py.py
***
import os

def collect_python_files(root_dir: str = ".", output_file: str = "collected_code.txt", exclude_dirs: set = None):
    if exclude_dirs is None:
        exclude_dirs = {"venv", "__pycache__"}

    with open(output_file, "w", encoding="utf-8") as out_f:
        for dirpath, dirnames, filenames in os.walk(root_dir):
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∏—Å–∫–ª—é—á—ë–Ω–Ω—ã–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
            dirnames[:] = [d for d in dirnames if d not in exclude_dirs]
            for filename in sorted(filenames):
                if filename.endswith(".py"):
                    filepath = os.path.join(dirpath, filename)
                    rel_path = os.path.relpath(filepath, root_dir)
                    try:
                        with open(filepath, "r", encoding="utf-8") as f:
                            content = f.read()
                        out_f.write(f"***\n{rel_path}\n***\n{content}\n***\n")
                    except Exception as e:
                        out_f.write(f"***\n{rel_path}\n***\n<–û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ —Ñ–∞–π–ª–∞: {e}>\n***\n")

if __name__ == "__main__":
    collect_python_files()
***
***
src/__init__.py
***

***
***
src/cli.py
***
#!/usr/bin/env python3
"""CLI –¥–ª—è pipeline –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤—Å—Ç—Ä–µ—á."""
import argparse
from pathlib import Path
from src.pipeline.main import main as run_pipeline

def main():
    parser = argparse.ArgumentParser(description="Pipeline –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –≤—Å—Ç—Ä–µ—á")
    parser.add_argument("audio_file", nargs="?", type=Path, 
                        help="–ü—É—Ç—å –∫ –∞—É–¥–∏–æ—Ñ–∞–π–ª—É (–æ–±—è–∑–∞—Ç–µ–ª–µ–Ω, –µ—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω --json)")
    parser.add_argument("--json", type=Path, 
                        help="–ü—É—Ç—å –∫ JSON –¥–ª—è –ø–µ—Ä–µ–∞–Ω–∞–ª–∏–∑–∞ (–ø—Ä–æ–ø—É—Å–∫–∞–µ—Ç —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—é –∏ –∑–∞–ø–∏—Å—å –≤ –ë–î)")
    parser.add_argument("--device", choices=["cuda", "cpu"], default="cuda")
    parser.add_argument("--no-db", action="store_true", 
                        help="–ë–µ–∑ –∑–∞–ø–∏—Å–∏ –≤ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö (—Ç–æ–ª—å–∫–æ —Ñ–∞–π–ª—ã). –ò–≥–Ω–æ—Ä–∏—Ä—É–µ—Ç—Å—è –ø—Ä–∏ --json")

    args = parser.parse_args()

    # –í–∞–ª–∏–¥–∞—Ü–∏—è: —Ä–æ–≤–Ω–æ –æ–¥–∏–Ω –∏—Å—Ç–æ—á–Ω–∏–∫
    if not args.audio_file and not args.json:
        parser.error("–£–∫–∞–∂–∏—Ç–µ –∞—É–¥–∏–æ—Ñ–∞–π–ª –ò–õ–ò --json")
    if args.audio_file and args.json:
        parser.error("–£–∫–∞–∂–∏—Ç–µ –¢–û–õ–¨–ö–û –æ–¥–∏–Ω –∏—Å—Ç–æ—á–Ω–∏–∫: –∞—É–¥–∏–æ –ò–õ–ò JSON")

    # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –≤–∫–ª—é—á–µ–Ω–∏–µ --no-db –ø—Ä–∏ --json (–∑–∞–ø–∏—Å—å –≤ –ë–î –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞)
    effective_no_db = True if args.json else args.no_db

    # –ó–∞–ø—É—Å–∫ –ø–∞–π–ø–ª–∞–π–Ω–∞
    run_pipeline(
        audio_file=str(args.audio_file) if args.audio_file else None,
        json_file=str(args.json) if args.json else None,
        device=args.device,
        no_db=effective_no_db
    )

if __name__ == "__main__":
    main()
***
***
src/cli_v2.py
***
import argparse
from pathlib import Path

from src.worker.worker import Worker
from src.jobs.models import Job


def main():
    parser = argparse.ArgumentParser("Meeting pipeline (job-based)")
    parser.add_argument("source", type=Path)
    parser.add_argument("--json", action="store_true")

    args = parser.parse_args()

    job = Job(
        source_type="json" if args.json else "audio",
        source_path=str(args.source),
    )

    worker = Worker()
    result = worker.submit(job)

    print("\n=== JOB RESULT ===")
    print(f"id: {result.id}")
    print(f"status: {result.status}")
    if result.error:
        print(f"error: {result.error}")


if __name__ == "__main__":
    main()

***
***
src/config/models.py
***
"""
–ó–∞–≥—Ä—É–∑—á–∏–∫ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –º–æ–¥–µ–ª–µ–π –∏–∑ models.yaml
"""
import os
import yaml
from pathlib import Path
from typing import Dict, Any, Optional
from dotenv import load_dotenv

load_dotenv()

class ModelConfigError(Exception):
    """–û—à–∏–±–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –º–æ–¥–µ–ª–∏"""
    pass

class ModelProfile:
    """–ü—Ä–æ—Ñ–∏–ª—å –º–æ–¥–µ–ª–∏ —Å –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π"""
    def __init__(self, key: str, config: Dict[str, Any]):
        self.key = key
        self.backend = config.get('backend')
        self.name = config.get('name')
        self.path = config.get('path')
        self.description = config.get('description', '')
        self.params = config.get('params', {})
        
        # –í–∞–ª–∏–¥–∞—Ü–∏—è –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –ø–æ–ª–µ–π
        if not self.backend:
            raise ModelConfigError(f"–ü—Ä–æ—Ñ–∏–ª—å '{key}': –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –ø–æ–ª–µ 'backend'")
        
        if self.backend == 'ollama' and not self.name:
            raise ModelConfigError(f"–ü—Ä–æ—Ñ–∏–ª—å '{key}': –¥–ª—è backend='ollama' —Ç—Ä–µ–±—É–µ—Ç—Å—è –ø–æ–ª–µ 'name'")
        if self.backend == 'llama_cpp' and not self.path:
            raise ModelConfigError(f"–ü—Ä–æ—Ñ–∏–ª—å '{key}': –¥–ª—è backend='llama_cpp' —Ç—Ä–µ–±—É–µ—Ç—Å—è –ø–æ–ª–µ 'path'")

    def __repr__(self):
        return f"<ModelProfile key={self.key} backend={self.backend}>"

class ModelsConfig:
    """–ö–æ–Ω—Ç–µ–π–Ω–µ—Ä –¥–ª—è –≤—Å–µ—Ö –ø—Ä–æ—Ñ–∏–ª–µ–π –º–æ–¥–µ–ª–µ–π"""
    def __init__(self, config_path: str = "models.yaml"):
        self.config_path = Path(config_path)
        self.profiles: Dict[str, ModelProfile] = {}
        self.default_model: str = ""
        self._load()
    
    def _load(self):
        if not self.config_path.exists():
            raise ModelConfigError(f"–§–∞–π–ª –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω: {self.config_path}")
        
        with open(self.config_path, 'r', encoding='utf-8') as f:
            raw_config = yaml.safe_load(f)
        
        self.default_model = raw_config.get('default_model', '')
        profiles_raw = raw_config.get('profiles', {})
        
        if not profiles_raw:
            raise ModelConfigError("–í –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –ø—Ä–æ—Ñ–∏–ª–∏ –º–æ–¥–µ–ª–µ–π")
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –≤—Å–µ—Ö –ø—Ä–æ—Ñ–∏–ª–µ–π
        for key, cfg in profiles_raw.items():
            try:
                self.profiles[key] = ModelProfile(key, cfg)
            except ModelConfigError as e:
                print(f"‚ö†Ô∏è –ü—Ä–æ–ø—É—â–µ–Ω –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π –ø—Ä–æ—Ñ–∏–ª—å '{key}': {e}")
    
    def get_active_profile(self) -> ModelProfile:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—Ä–æ—Ñ–∏–ª—å, —É–∫–∞–∑–∞–Ω–Ω—ã–π –≤ ACTIVE_MODEL_PROFILE –∏–ª–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é"""
        active_key = os.getenv('ACTIVE_MODEL_PROFILE', self.default_model)
        
        if not active_key:
            raise ModelConfigError(
                "–ù–µ —É–∫–∞–∑–∞–Ω–∞ –∞–∫—Ç–∏–≤–Ω–∞—è –º–æ–¥–µ–ª—å. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ ACTIVE_MODEL_PROFILE –≤ .env "
                "–∏–ª–∏ –∑–∞–¥–∞–π—Ç–µ default_model –≤ models.yaml"
            )
        
        profile = self.profiles.get(active_key)
        if not profile:
            available = ', '.join(self.profiles.keys())
            raise ModelConfigError(
                f"–ü—Ä–æ—Ñ–∏–ª—å '{active_key}' –Ω–µ –Ω–∞–π–¥–µ–Ω. –î–æ—Å—Ç—É–ø–Ω—ã–µ –ø—Ä–æ—Ñ–∏–ª–∏: {available}"
            )
        
        return profile
    
    def list_profiles(self) -> Dict[str, str]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å {–∫–ª—é—á: –æ–ø–∏—Å–∞–Ω–∏–µ} –¥–ª—è –≤—Å–µ—Ö –ø—Ä–æ—Ñ–∏–ª–µ–π"""
        return {key: profile.description for key, profile in self.profiles.items()}

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä –¥–ª—è –∏–º–ø–æ—Ä—Ç–∞
_models_config: Optional[ModelsConfig] = None

def get_models_config() -> ModelsConfig:
    """–õ–µ–Ω–∏–≤–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"""
    global _models_config
    if _models_config is None:
        _models_config = ModelsConfig()
    return _models_config
***
***
src/contracts/analysis_result.py
***
from dataclasses import dataclass
from datetime import datetime
from typing import List


@dataclass
class AnalysisResult:
    """
    Stable contract for analysis output.
    """

    prompt_id: str
    generated_at: datetime

    summary_raw: str
    segment_count: int

    model_backend: str
    model_profile: str

***
***
src/worker/__init__.py
***

***
***
src/worker/worker.py
***
from src.jobs.runner import JobRunner
from src.jobs.postgres_repository import PostgresJobRepository
from src.jobs.models import Job

from src.llm.adapter import LLMAdapter
from src.pipeline_v2.services import Services


class Worker:
    def __init__(self):
        # --- infrastructure ---
        self.repo = PostgresJobRepository()

        # üîë LLM INITIALIZED ONCE PER WORKER
        llm_adapter = LLMAdapter(models_config_path="models.yaml")

        self.services = Services(
            llm=llm_adapter
        )

        # --- runner ---
        self.runner = JobRunner(self.repo, self.services)

    def submit(self, job: Job) -> Job:
        self.repo.save(job)
        self.runner.run(job)
        return job

***
***
src/legacy_llm/mistral.py
***
from llama_cpp import Llama


class MistralClient:
    def __init__(self):
        self.model_name = "mistral_8b_q5km"
        self.llm = Llama(
            model_path="/home/dsefros/models_ai/Ministral-8B-Instruct-2410-Q5_K_M.gguf",
            n_ctx=16384,
            n_threads=8,
            n_gpu_layers=37,
            verbose=False,
        )

    def generate(self, prompt: str) -> str:
        output = self.llm(
            prompt,
            max_tokens=512,
            temperature=0.2,
            top_p=0.9,
            stop=["</s>"],
        )

        return output["choices"][0]["text"].strip()

***
***
src/jobs/job_step_repository.py
***
from uuid import uuid4
from datetime import datetime
import json

from sqlalchemy import text

from src.jobs.models import JobStep, StepStatus
from src.storage.postgres import init_db


class JobStepRepository:
    """
    Control-plane repository for pipeline steps.
    –ù–ï –ø—É—Ç–∞—Ç—å —Å JobRepository.
    """

    def __init__(self):
        self.engine = init_db()

    def get(self, job_id, step_name):
        with self.engine.connect() as conn:
            row = conn.execute(
                text("""
                SELECT *
                FROM job_steps
                WHERE job_id = :job_id
                  AND step_name = :step_name
                """),
                {"job_id": job_id, "step_name": step_name},
            ).mappings().first()

        if not row:
            return None

        return JobStep(
            id=row["id"],
            job_id=row["job_id"],
            step_name=row["step_name"],
            status=StepStatus(row["status"]),
            attempt=row["attempt"],
            artifacts=row["artifacts"],
            error=row["error"],
            created_at=row["created_at"],
            updated_at=row["updated_at"],
        )

    def create_if_not_exists(self, job_id, step_name):
        step = self.get(job_id, step_name)
        if step:
            return step

        step_id = uuid4()
        with self.engine.begin() as conn:
            conn.execute(
                text("""
                INSERT INTO job_steps (
                    id, job_id, step_name, status, attempt
                )
                VALUES (
                    :id, :job_id, :step_name, :status, 0
                )
                """),
                {
                    "id": step_id,
                    "job_id": job_id,
                    "step_name": step_name,
                    "status": StepStatus.PENDING.value,
                },
            )

        return self.get(job_id, step_name)

    def mark_running(self, step):
        with self.engine.begin() as conn:
            conn.execute(
                text("""
                UPDATE job_steps
                SET status = :status,
                    attempt = attempt + 1,
                    updated_at = :updated_at
                WHERE id = :id
                """),
                {
                    "id": step.id,
                    "status": StepStatus.RUNNING.value,
                    "updated_at": datetime.utcnow(),
                },
            )

    import json

    def mark_completed(self, step, artifacts):
        with self.engine.begin() as conn:
            conn.execute(
                text("""
                UPDATE job_steps
                SET status = :status,
                    artifacts = CAST(:artifacts AS JSONB),
                    error = NULL,
                    updated_at = :updated_at
                WHERE id = :id
                """),
                {
                    "id": step.id,
                    "status": StepStatus.COMPLETED.value,
                    "artifacts": json.dumps(artifacts),
                    "updated_at": datetime.utcnow(),
                },
            )


    def mark_failed(self, step, error):
        with self.engine.begin() as conn:
            conn.execute(
                text("""
                UPDATE job_steps
                SET status = :status,
                    error = :error,
                    updated_at = :updated_at
                WHERE id = :id
                """),
                {
                    "id": step.id,
                    "status": StepStatus.FAILED.value,
                    "error": error,
                    "updated_at": datetime.utcnow(),
                },
            )

***
***
src/jobs/models.py
***
from dataclasses import dataclass, field
from enum import Enum
from uuid import UUID, uuid4
from datetime import datetime


class JobStatus(str, Enum):
    PENDING = "pending"
    RUNNING = "running"
    FAILED = "failed"
    COMPLETED = "completed"


@dataclass
class Job:
    id: UUID = field(default_factory=uuid4)
    source_type: str = ""
    source_path: str = ""
    status: JobStatus = JobStatus.PENDING

    current_step: str | None = None
    error: str | None = None
    attempt: int = 0            # ‚Üê –í–û–¢ –≠–¢–û –ü–û–õ–ï

    created_at: datetime = field(default_factory=datetime.utcnow)
    updated_at: datetime = field(default_factory=datetime.utcnow)

from dataclasses import dataclass
from enum import Enum
from uuid import UUID
from datetime import datetime
from typing import Optional, Dict, Any


class StepStatus(str, Enum):
    PENDING = "pending"
    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"


@dataclass
class JobStep:
    id: UUID
    job_id: UUID
    step_name: str

    status: StepStatus
    attempt: int

    artifacts: Optional[Dict[str, Any]]
    error: Optional[str]

    created_at: datetime
    updated_at: datetime

***
***
src/jobs/postgres_repository.py
***
from datetime import datetime
from uuid import UUID

from sqlalchemy import text
from src.jobs.models import Job, JobStatus
from src.storage.postgres import init_db


class PostgresJobRepository:
    def __init__(self):
        self.engine = init_db()

    def save(self, job: Job):
        with self.engine.begin() as conn:
            conn.execute(
                text("""
                INSERT INTO jobs (
                    id, source_type, source_path,
                    status, current_step, error,
                    attempt, created_at, updated_at
                )
                VALUES (
                    :id, :source_type, :source_path,
                    :status, :current_step, :error,
                    :attempt, :created_at, :updated_at
                )
                """),
                {
                    "id": job.id,
                    "source_type": job.source_type,
                    "source_path": job.source_path,
                    "status": job.status.value,
                    "current_step": job.current_step,
                    "error": job.error,
                    "attempt": job.attempt,
                    "created_at": job.created_at,
                    "updated_at": job.updated_at,
                }
            )

    def update(self, job: Job):
        with self.engine.begin() as conn:
            conn.execute(
                text("""
                UPDATE jobs
                SET status = :status,
                    current_step = :current_step,
                    error = :error,
                    attempt = :attempt,
                    updated_at = :updated_at
                WHERE id = :id
                """),
                {
                    "id": job.id,
                    "status": job.status.value,
                    "current_step": job.current_step,
                    "error": job.error,
                    "attempt": job.attempt,
                    "updated_at": datetime.utcnow(),
                }

            )

    def get(self, job_id: UUID) -> Job | None:
        with self.engine.connect() as conn:
            row = conn.execute(
                text("SELECT * FROM jobs WHERE id = :id"),
                {"id": job_id}
            ).mappings().first()

        if not row:
            return None

        return Job(
            id=row["id"],
            source_type=row["source_type"],
            source_path=row["source_path"],
            status=JobStatus(row["status"]),
            current_step=row["current_step"],
            error=row["error"],
            attempt=row["attempt"],
            created_at=row["created_at"],
            updated_at=row["updated_at"],
        )

***
***
src/jobs/repository.py
***
from typing import Dict
from src.jobs.models import Job


class InMemoryJobRepository:
    def __init__(self):
        self._jobs: Dict[str, Job] = {}

    def save(self, job: Job):
        self._jobs[str(job.id)] = job

    def get(self, job_id: str) -> Job | None:
        return self._jobs.get(job_id)

    def update(self, job: Job):
        self._jobs[str(job.id)] = job

***
***
src/jobs/runner.py
***
from datetime import datetime
from pathlib import Path
import traceback

from src.jobs.models import Job, JobStatus
from src.pipeline_v2.context import PipelineContext
from src.pipeline_v2.orchestrator import PipelineOrchestrator
from src.pipeline_v2.services import Services


class JobRunner:
    def __init__(self, repo, services: Services):
        self.repo = repo
        self.services = services

    def run(self, job: Job):
        try:
            # --- job start ---
            job.status = JobStatus.RUNNING
            job.current_step = None
            job.error = None
            job.updated_at = datetime.utcnow()
            self.repo.update(job)

            # --- pipeline context ---
            ctx = PipelineContext(
                job_id=job.id,
                source_type=job.source_type,
                source_path=Path(job.source_path),
                services=self.services,      # üîë inject services
            )

            # --- PIPELINE ---
            orchestrator = PipelineOrchestrator()
            orchestrator.run(job, ctx)

            # --- job success ---
            job.status = JobStatus.COMPLETED
            job.updated_at = datetime.utcnow()
            self.repo.update(job)

        except Exception as e:
            job.status = JobStatus.FAILED
            job.error = str(e)
            job.updated_at = datetime.utcnow()
            self.repo.update(job)

            traceback.print_exc()

***
***
src/jobs/worker.py
***
from src.jobs.runner import JobRunner
from src.jobs.repository import InMemoryJobRepository
from src.jobs.models import Job


class Worker:
    def __init__(self):
        self.repo = InMemoryJobRepository()
        self.runner = JobRunner(self.repo)

    def submit(self, job: Job):
        self.repo.save(job)
        self.runner.run(job)
        return job

***
***
src/prompts/registry.py
***
from pathlib import Path
import yaml


class PromptNotFound(Exception):
    pass


class PromptRegistry:
    """
    Loads and renders versioned prompts.
    """

    def __init__(self, base_dir: str = "src/prompts"):
        self.base_dir = Path(base_dir)

    def load(self, relative_path: str) -> dict:
        """
        Example: analysis/v1.yaml
        """
        path = self.base_dir / relative_path

        if not path.exists():
            raise PromptNotFound(f"Prompt not found: {path}")

        with open(path, "r", encoding="utf-8") as f:
            return yaml.safe_load(f)

    def render(self, relative_path: str, **variables) -> str:
        prompt = self.load(relative_path)

        template = prompt.get("template")
        if not template:
            raise ValueError("Prompt template missing")

        for key, value in variables.items():
            template = template.replace(f"{{{{ {key} }}}}", value)

        return template

***
***
src/utils/term_loader.py
***
from qdrant_client import QdrantClient
from qdrant_client.http import models
from sentence_transformers import SentenceTransformer
import uuid

client = QdrantClient(host='localhost', port=6333)
encoder = SentenceTransformer('all-MiniLM-L6-v2')

term_data = {
    'term_id': 'tms',
    'term_name': 'TMS',
    'full_name': 'Terminal Management System',
    'definition': '–ü—Ä–æ–≥—Ä–∞–º–º–Ω–æ–µ –æ–±–µ—Å–ø–µ—á–µ–Ω–∏–µ –¥–ª—è —Ü–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–≥–æ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ø–∞—Ä–∫–æ–º POS-—Ç–µ—Ä–º–∏–Ω–∞–ª–æ–≤. –û–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç —É–¥–∞–ª–µ–Ω–Ω—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é, –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ —Å–æ—Å—Ç–æ—è–Ω–∏—è, –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø—Ä–æ—à–∏–≤–æ–∫ –∏ —Å–±–æ—Ä –¥–∏–∞–≥–Ω–æ—Å—Ç–∏—á–µ—Å–∫–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ —Å —Ç–µ—Ä–º–∏–Ω–∞–ª–æ–≤.',
    'related_terms': ['–ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö', 'POS-—Ç–µ—Ä–º–∏–Ω–∞–ª', '–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è', '–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥', '–õ–∏—Ü–µ–Ω–∑–∏—Ä–æ–≤–∞–Ω–∏–µ', 'Somers.POS'],
    'business_context': ['—É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Å–µ—Ç—å—é —Ç–µ—Ä–º–∏–Ω–∞–ª–æ–≤', '–†–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏–µ –Ω–æ–≤—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π –Ω–∞ —Ç–µ—Ä–º–∏–Ω–∞–ª–∞—Ö', '–î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –ø—Ä–æ–±–ª–µ–º —Å —Ç–µ—Ä–º–∏–Ω–∞–ª–∞–º–∏', '–û–±–µ—Å–ø–µ—á–µ–Ω–∏–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º –ù–°–ü–ö'],
    'regulatory_info': '–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º —Ä–µ–≥—É–ª—è—Ç–æ—Ä–æ–≤ –ø–æ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø–ª–∞—Ç–µ–∂–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏ —É–¥–∞–ª–µ–Ω–Ω–æ–º—É —É–ø—Ä–∞–≤–ª–µ–Ω–∏—é —Ç–µ—Ä–º–∏–Ω–∞–ª–∞–º–∏.',
    'common_misconceptions': [],
    'examples': [
        '–¢–µ—Ä–º–∏–Ω–∞–ª —Å—Ö–æ–¥–∏–ª –Ω–∞ TMS –∏ –ø–æ–ª—É—á–∏–ª –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é.',
        '–¢–µ—Ä–º–∏–Ω–∞–ª –∏—Å–ø–æ–ª—å–∑—É–µ—Ç TMS, —á—Ç–æ-–±—ã –æ–±–Ω–æ–≤–ª—è—Ç—å —Å–≤–æ–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã.',
        '–ù–∞ TMS –º–æ–∂–Ω–æ –æ—Ç—Å–ª–µ–∂–∏–≤–∞—Ç—å —Ä–∞–±–æ—Ç—É —Ç–µ—Ä–º–∏–Ω–∞–ª–æ–≤.',
        '–ò–Ω–∂–µ–Ω–µ—Ä –°–æ–º–µ—Ä—Å –∑–∞—Ö–æ–¥–∏—Ç –≤ TMS –¥–ª—è –≤—ã—Å—Ç–∞–≤–ª–µ–Ω–∏—è –Ω–∞—Å—Ç—Ä–æ–µ–∫ –°–ë–ü –Ω–∞ –Ω–æ–≤—ã–µ —Ç–µ—Ä–º–∏–Ω–∞–ª—ã'
    ],
    'importance_level': 9,
    'last_updated': '2026-01-14'
}

embedding = encoder.encode(term_data['definition']).tolist()
# –ò—Å–ø–æ–ª—å–∑—É–µ–º UUID –≤–º–µ—Å—Ç–æ —Å—Ç—Ä–æ–∫–∏
point_id = str(uuid.uuid4())

client.upsert(
    collection_name='technical_terms',
    points=[models.PointStruct(id=point_id, vector=embedding, payload=term_data)]
)
print(f'‚úÖ TMS –¥–æ–±–∞–≤–ª–µ–Ω –≤ Qdrant! ID: {point_id}')

***
***
src/pipeline__deprecated/main.py
***
#!/usr/bin/env python3
"""
–ï–¥–∏–Ω—ã–π pipeline –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤—Å—Ç—Ä–µ—á:
1. –¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—è + –¥–∏–∞—Ä–∏–∑–∞—Ü–∏—è —á–µ—Ä–µ–∑ WhisperX (–µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω –∞—É–¥–∏–æ—Ñ–∞–π–ª)
2. –ê–Ω–∞–ª–∏–∑ —á–µ—Ä–µ–∑ –ª–æ–∫–∞–ª—å–Ω—É—é LLM (Ollama/llama-cpp)
3. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –æ—Ç—á—ë—Ç–∞ –≤ Markdown
4. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –≤ Postgres –∏ Qdrant (—Ç–æ–ª—å–∫–æ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∞—É–¥–∏–æ)

–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç offline-—Ä–µ–∂–∏–º –±–µ–∑ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–∞.
"""
import os
import sys
import json
import gc
from pathlib import Path
from dotenv import load_dotenv
import torch
import ollama
from datetime import datetime
import hashlib
import traceback

# === –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ ===
load_dotenv()
INPUT_DIR = os.getenv("INPUT_DIR", "input")
OUTPUT_DIR = os.getenv("OUTPUT_DIR", "output")
HF_TOKEN = os.getenv("HF_TOKEN")
os.makedirs(OUTPUT_DIR, exist_ok=True)

# === –ò–º–ø–æ—Ä—Ç –º–æ–¥—É–ª–µ–π –±–∞–∑ –¥–∞–Ω–Ω—ã—Ö ===
from src.storage.postgres import init_db, get_db_session, Meeting, Speaker, Fragment
from src.storage.qdrant import init_qdrant_client, create_collections_if_not_exists

# === –ò–º–ø–æ—Ä—Ç –Ω–æ–≤–æ–≥–æ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ ===
from src.config.models import get_models_config
from src.ai.generator import generate_text

def _free_gpu_memory():
    """–ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–µ –æ—Å–≤–æ–±–æ–∂–¥–µ–Ω–∏–µ –≤—Å–µ–π –¥–æ—Å—Ç—É–ø–Ω–æ–π –ø–∞–º—è—Ç–∏ –ø–æ—Å–ª–µ —ç—Ç–∞–ø–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.synchronize()
    gc.collect()
    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞ PyTorch
    if hasattr(torch.cuda, 'cudnn'):
        torch.backends.cudnn.benchmark = False
    if torch.cuda.is_available():
        print(f"[DEBUG] VRAM –ø–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏: {torch.cuda.memory_allocated() / 1024**2:.1f} / {torch.cuda.get_device_properties(0).total_memory / 1024**2:.1f} MB")

# === –ù–û–í–ê–Ø –§–£–ù–ö–¶–ò–Ø: –∑–∞–≥—Ä—É–∑–∫–∞ —Å–µ–≥–º–µ–Ω—Ç–æ–≤ –∏–∑ JSON ===
def load_segments_from_json(json_path: str) -> tuple[list, str, str, float]:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å–µ–≥–º–µ–Ω—Ç—ã –∏–∑ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ JSON.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç: (—Å–µ–≥–º–µ–Ω—Ç—ã, –∏—Å—Ö–æ–¥–Ω–æ–µ –∏–º—è —Ñ–∞–π–ª–∞, –∞—É–¥–∏–æ—Ö–µ—à, –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å)"""
    with open(json_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    if 'transcription' not in data:
        raise ValueError(f"JSON –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –ø–æ–ª—è 'transcription': {json_path}")
    
    segments = [
        {
            'speaker': seg['speaker'],
            'text': seg['text'],
            'start': seg['start'],
            'end': seg['end']
        }
        for seg in data['transcription']
    ]
    
    metadata = data.get('metadata', {})
    orig_filename = metadata.get('filename', Path(json_path).stem)
    audio_hash = metadata.get('audio_hash', hashlib.sha256(Path(json_path).name.encode()).hexdigest()[:16])
    duration = metadata.get('duration_sec', sum(seg['end'] - seg['start'] for seg in segments))
    
    return segments, orig_filename, audio_hash, duration

# === 1. –¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—è –∏ –¥–∏–∞—Ä–∏–∑–∞—Ü–∏—è (–Ω–∞ –æ—Å–Ω–æ–≤–µ transcribe_v3.py) ===
def transcribe_and_diarize(audio_path: str, device: str = "cuda"):
    """–¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—è + –¥–∏–∞—Ä–∏–∑–∞—Ü–∏—è —á–µ—Ä–µ–∑ WhisperX —Å –≥–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –æ—Å–≤–æ–±–æ–∂–¥–µ–Ω–∏–µ–º –ø–∞–º—è—Ç–∏."""
    import whisperx
    from pydub import AudioSegment
    
    print(f"[DEBUG] –ó–∞–≥—Ä—É–∑–∫–∞ WhisperX (large-v3) –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ: {device}")
    model = whisperx.load_model("large-v3", device, compute_type="float16" if device == "cuda" else "int8")
    
    # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ WAV 16kHz –º–æ–Ω–æ
    print("[DEBUG] –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –∞—É–¥–∏–æ –≤ WAV...")
    audio = AudioSegment.from_file(audio_path)
    audio = audio.set_channels(1).set_frame_rate(16000)
    wav_path = "temp_audio.wav"
    audio.export(wav_path, format="wav")
    
    # –¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—è
    print("[DEBUG] –¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—è...")
    audio_data = whisperx.load_audio(wav_path)
    result = model.transcribe(audio_data, language="ru")
    
    # –í—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ ‚Äî –∑–∞–≥—Ä—É–∂–∞–µ–º –¢–û–õ–¨–ö–û –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
    model_a, metadata = whisperx.load_align_model(
        language_code="ru",
        device=device,
        model_name="facebook/wav2vec2-base-960h"
    )
    result = whisperx.align(result["segments"], model_a, metadata, audio_data, device)
    
    # –û—Å–≤–æ–±–æ–∂–¥–∞–µ–º –º–æ–¥–µ–ª—å –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è –°–†–ê–ó–£ –ø–æ—Å–ª–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
    del model_a
    _free_gpu_memory()
    
    # –î–∏–∞—Ä–∏–∑–∞—Ü–∏—è
    print("[DEBUG] –î–∏–∞—Ä–∏–∑–∞—Ü–∏—è...")
    diarize_model = whisperx.DiarizationPipeline(use_auth_token=HF_TOKEN, device=device)
    diarize_segments = diarize_model(audio_data)
    result = whisperx.assign_word_speakers(diarize_segments, result)
    
    # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–µ–≥–º–µ–Ω—Ç–æ–≤
    segments = []
    for seg in result["segments"]:
        segments.append({
            "text": seg.get("text", "").strip(),
            "start": round(seg.get("start", 0), 2),
            "end": round(seg.get("end", 0), 2),
            "speaker": seg.get("speaker", "SPEAKER_00")
        })
    
    # üî• –ö–†–ò–¢–ò–ß–ù–û: –æ—Å–≤–æ–±–æ–∂–¥–∞–µ–º –í–°–ï –º–æ–¥–µ–ª–∏ –∏ –¥–∞–Ω–Ω—ã–µ —ç—Ç–∞–ø–∞ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏–∏
    del model
    del diarize_model
    del audio_data
    del result
    del diarize_segments
    _free_gpu_memory()
    
    # –û—á–∏—Å—Ç–∫–∞ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
    if os.path.exists(wav_path):
        os.remove(wav_path)
    
    print(f"[DEBUG] WhisperX –∑–∞–≤–µ—Ä—à—ë–Ω. –°–µ–≥–º–µ–Ω—Ç–æ–≤: {len(segments)}")
    return segments

# === 2. –ê–Ω–∞–ª–∏–∑ —á–µ—Ä–µ–∑ Ollama (–ª–æ–∫–∞–ª—å–Ω–∞—è LLM) ===
def analyze_with_model(segments: list) -> str:
    """–ê–Ω–∞–ª–∏–∑ –¥–∏–∞–ª–æ–≥–∞ —á–µ—Ä–µ–∑ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å (Ollama –∏–ª–∏ llama-cpp)."""
    # –ó–∞–≥—Ä—É–∑–∫–∞ –∞–∫—Ç–∏–≤–Ω–æ–≥–æ –ø—Ä–æ—Ñ–∏–ª—è –º–æ–¥–µ–ª–∏
    models_cfg = get_models_config()
    profile = models_cfg.get_active_profile()
    print(f"[DEBUG] –ê–Ω–∞–ª–∏–∑ —á–µ—Ä–µ–∑ {profile.backend} ({profile.key})...")
    
    # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∏–∞–ª–æ–≥–∞ –¥–ª—è –ø—Ä–æ–º–ø—Ç–∞
    dialogue = "\n".join([f"{seg['speaker']}: {seg['text']}" for seg in segments if seg["text"]])
    
    # –°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç
    system_prompt = f"""
–¢—ã ‚Äî —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∞–Ω–∞–ª–∏–∑—É —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –≤—Å—Ç—Ä–µ—á –≤ –∫–æ–º–ø–∞–Ω–∏–∏ –°–æ–º–º–µ—Ä—Å. –°–æ–º–º–µ—Ä—Å —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç—Å—è –Ω–∞ IT-—Ä–µ—à–µ–Ω–∏—è—Ö –¥–ª—è —ç–∫–≤–∞–π—Ä–∏–Ω–≥–∞, POS-—Ç–µ—Ä–º–∏–Ω–∞–ª–æ–≤ –∏ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –±–∏–∑–Ω–µ—Å–∞. –¢–≤–æ–∏ –∫–ª–∏–µ–Ω—Ç—ã ‚Äî –±–∞–Ω–∫–∏, —Ä–∏—Ç–µ–π–ª –∏ —á–∞—Å—Ç–Ω—ã–µ –º–µ—Ä—á–∞–Ω—Ç—ã.
–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –¥–∏–∞–ª–æ–≥ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤—å –°–¢–†–£–ö–¢–£–†–ò–†–û–í–ê–ù–ù–´–ô –û–¢–ß–Å–¢ –≤ —Ñ–æ—Ä–º–∞—Ç–µ Markdown —Å–æ —Å–ª–µ–¥—É—é—â–∏–º–∏ —Ä–∞–∑–¥–µ–ª–∞–º–∏:
### üìù –ö—Ä–∞—Ç–∫–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ
- –û–±—â–∞—è —Ç–µ–º–∞ –≤—Å—Ç—Ä–µ—á–∏ (1-2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è)
### ‚ö†Ô∏è –ü—Ä–æ–±–ª–µ–º—ã –∏ —Ä–µ—à–µ–Ω–∏—è
–î–ª—è –∫–∞–∂–¥–æ–π —Ç–µ–º—ã –æ–±—Å—É–∂–¥–µ–Ω–∏—è:
- –ù–∞–∑–≤–∞–Ω–∏–µ –ø—Ä–æ–±–ª–µ–º—ã (–∫—Ä–∞—Ç–∫–æ)
- –ö—Ç–æ –æ–∑–≤—É—á–∏–ª –ø—Ä–æ–±–ª–µ–º—É (—Å–ø–∏–∫–µ—Ä)
- –°—É—Ç—å –ø—Ä–æ–±–ª–µ–º—ã
- –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–µ —Ä–µ—à–µ–Ω–∏—è (–µ—Å–ª–∏ –µ—Å—Ç—å)
- –ö—Ç–æ –ø—Ä–µ–¥–ª–æ–∂–∏–ª —Ä–µ—à–µ–Ω–∏–µ (—Å–ø–∏–∫–µ—Ä)
- –°—Ç–∞—Ç—É—Å —Ä–µ—à–µ–Ω–∏—è (—Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–æ/–≤ —Ä–∞–±–æ—Ç–µ/—Ç—Ä–µ–±—É–µ—Ç —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–∏—è)
–í–ê–ñ–ù–û:
1. –ï—Å–ª–∏ –≤ –¥–∏–∞–ª–æ–≥–µ –µ—Å—Ç—å —É–ø–æ–º–∏–Ω–∞–Ω–∏—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö –¥–∞—Ç ‚Äî –∏—Å–ø–æ–ª—å–∑—É–π –∏—Ö.
2. –ï—Å–ª–∏ –¥–∞—Ç—ã –Ω–µ—Ç, –Ω–æ –µ—Å—Ç—å —É–∫–∞–∑–∞–Ω–∏–µ –Ω–∞ —Å—Ä–æ–∫ ("—Å–µ–≥–æ–¥–Ω—è", "–Ω–∞ —Å–ª–µ–¥—É—é—â–µ–π –Ω–µ–¥–µ–ª–µ") ‚Äî –ø—Ä–µ–æ–±—Ä–∞–∑—É–π –≤ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—É—é –¥–∞—Ç—É –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ —Ç–µ–∫—É—â–µ–π –¥–∞—Ç—ã {datetime.now().strftime('%d.%m.%Y')}.
3. –§–æ–∫—É—Å–∏—Ä—É–π—Å—è –Ω–∞ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –¥–µ—Ç–∞–ª—è—Ö: –°–ë–ü, —Ç–µ—Ä–º–∏–Ω–∞–ª—ã (Ingenico/Newland), –ø—Ä–æ—à–∏–≤–∫–∏, –≤–æ–∑–≤—Ä–∞—Ç—ã, —Å—Ç–µ–π—Ç-—Ö–æ–ª–¥–µ—Ä—ã, –ª–æ–≥–∏.
4. –ù–µ –ø—Ä–∏–¥—É–º—ã–≤–∞–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é, –∫–æ—Ç–æ—Ä–æ–π –Ω–µ—Ç –≤ –¥–∏–∞–ª–æ–≥–µ.
5. –ï—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –Ω–µ–ø–æ–ª–Ω–∞—è ‚Äî –ø–æ–º–µ—á–∞–π —Å—Ç–∞—Ç—É—Å –∫–∞–∫ "–¢—Ä–µ–±—É–µ—Ç —É—Ç–æ—á–Ω–µ–Ω–∏—è".
–û—Ç–≤–µ—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Å—Ç—Ä–æ–≥–æ –≤ —Ñ–æ—Ä–º–∞—Ç–µ Markdown –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤.
"""
    
    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è
    try:
        if profile.backend == "ollama":
            # –î–ª—è Ollama –∏—Å–ø–æ–ª—å–∑—É–µ–º —á–∞—Ç-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å —Å —Å–∏—Å—Ç–µ–º–Ω–æ–π —Ä–æ–ª—å—é
            response = ollama.chat(
                model=profile.name,
                messages=[
                    {"role": "system", "content": system_prompt.strip()},
                    {"role": "user", "content": f"–î–∏–∞–ª–æ–≥ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞:\n{dialogue}"}
                ],
                options={
                    "temperature": profile.params.get("temperature", 0.1),
                    "top_p": profile.params.get("top_p", 0.9),
                    "repeat_penalty": profile.params.get("repeat_penalty", 1.15),
                    "num_predict": profile.params.get("num_predict", 2000),
                },
                stream=False
            )
            return response["message"]["content"].strip()
        else:
            # –î–ª—è llama-cpp –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–¥–∏–Ω—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å generate_text()
            full_prompt = f"–¢—ã ‚Äî —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∞–Ω–∞–ª–∏–∑—É —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –≤—Å—Ç—Ä–µ—á –≤ –∫–æ–º–ø–∞–Ω–∏–∏ –°–æ–º–º–µ—Ä—Å.\n{system_prompt.strip()}\n–î–∏–∞–ª–æ–≥ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞:\n{dialogue}"
            return generate_text(full_prompt, profile)
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ: {e}")
        return f"‚ö†Ô∏è **–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞**\n–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –¥–∏–∞–ª–æ–≥ –∏–∑-–∑–∞ –æ—à–∏–±–∫–∏:\n`{str(e)}`\n\n**–°—ã—Ä—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–∏–∞–ª–æ–≥–∞:**\n{dialogue[:1000]}..."

# === 3. –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤ ===
def extract_technical_terms(text: str) -> list:
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤ –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
    technical_terms = [
        'TSP', '–ü–ò–æ–¢', '–ï–ì–ê–ò–°', '–º–∞—Ä–∫–∏—Ä–æ–≤–∫–∞', '–æ—Ñ–ª–∞–π–Ω', '–ß–µ—Å—Ç–Ω—ã–π –ó–ù–ê–ö',
        '–°–ë–ü', '—ç–∫–≤–∞–π—Ä–∏–Ω–≥', 'POS', '—Ç–µ—Ä–º–∏–Ω–∞–ª', '–ø—Ä–æ—à–∏–≤–∫–∞', '–≤–æ–∑–≤—Ä–∞—Ç—ã',
        '—Å—Ç–µ–π—Ç-—Ö–æ–ª–¥–µ—Ä—ã', '–ª–æ–≥–∏', 'API', '–∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è', '–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å',
        'Sommers', '–°–æ–º–º–µ—Ä—Å', '–∫–æ–Ω—Ç—Ä–∞–≥–µ–Ω—Ç', '–∫–ª–∏–µ–Ω—Ç', '–ø—Ä–æ–¥—É–∫—Ç', '—Ç–∞—Ä–∏—Ñ',
        'UTM', '—Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏—è', '–∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è', '–æ—Ç–∫–∞–∑', '–∫–æ–º–∏—Å—Å–∏—è'
    ]
    found_terms = []
    text_lower = text.lower()
    for term in technical_terms:
        if term.lower() in text_lower:
            found_terms.append(term)
    return found_terms

# === 4. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¢–û–õ–¨–ö–û –≤ —Ñ–∞–π–ª—ã (–±–µ–∑ –ë–î) ===
def save_to_file_only(filename: str, segments: list, analysis_md: str, audio_hash: str, duration_sec: float) -> str:
    """
    –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¢–û–õ–¨–ö–û –≤ —Ñ–∞–π–ª—ã (–±–µ–∑ –ë–î).
    –ü—Ä–∏–Ω–∏–º–∞–µ—Ç —Ö–µ—à –∏ –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –Ω–∞–ø—Ä—è–º—É—é ‚Äî –Ω–µ —Ç—Ä–µ–±—É–µ—Ç –¥–æ—Å—Ç—É–ø–∞ –∫ –∞—É–¥–∏–æ—Ñ–∞–π–ª—É.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—É—Ç—å –∫ –æ—Å–Ω–æ–≤–Ω–æ–º—É Markdown-—Ñ–∞–π–ª—É.
    """
    base_name = Path(filename).stem
    output_path = Path(OUTPUT_DIR)
    output_path.mkdir(exist_ok=True)
    
    # 1. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ Markdown-–æ—Ç—á—ë—Ç–∞
    md_path = output_path / f"{base_name}.md"
    with open(md_path, "w", encoding="utf-8") as f:
        f.write(analysis_md)
    print(f"üìÑ Markdown-–æ—Ç—á—ë—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {md_path}")
    
    # 2. –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ–ª–Ω–æ–≥–æ JSON-—Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
    json_result = {
        "metadata": {
            "filename": filename,
            "audio_hash": audio_hash,
            "processed_at": datetime.now().isoformat(),
            "duration_sec": duration_sec,
            "segment_count": len(segments),
            "reanalyzed_at": datetime.now().isoformat()  # –º–∞—Ä–∫–µ—Ä –ø–µ—Ä–µ–∞–Ω–∞–ª–∏–∑–∞
        },
        "transcription": [
            {
                "speaker": seg.get('speaker', 'SPEAKER_00'),
                "text": seg.get('text', '').strip(),
                "start": seg.get('start', 0),
                "end": seg.get('end', 0),
                "technical_terms": extract_technical_terms(seg.get('text', ''))
            }
            for seg in segments
        ],
        "analysis": {
            "raw_markdown": analysis_md,
            "extracted_terms": list(set(term for seg in segments for term in extract_technical_terms(seg.get('text', ''))))
        }
    }
    
    # 3. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ JSON (–ø–µ—Ä–µ–∑–∞–ø–∏—Å—å —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ —Ñ–∞–π–ª–∞)
    json_path = output_path / f"{base_name}.json"
    with open(json_path, "w", encoding="utf-8") as f:
        json.dump(json_result, f, ensure_ascii=False, indent=2)
    print(f"üì¶ JSON –æ–±–Ω–æ–≤–ª—ë–Ω: {json_path}")
    
    return str(md_path)

# === 5. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö (—Ç–æ–ª—å–∫–æ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∞—É–¥–∏–æ) ===
def save_to_databases(session, qdrant_client, filename, segments, analysis_md, original_audio_path):
    """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ Postgres –∏ Qdrant + –≤—Å–µ–≥–¥–∞ —Å–æ—Ö—Ä–∞–Ω—è—Ç—å —Ñ–∞–π–ª—ã –Ω–∞ –¥–∏—Å–∫"""
    print("üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –≤ –±–∞–∑—ã...")
    try:
        # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Ö–µ—à–∞ –∞—É–¥–∏–æ—Ñ–∞–π–ª–∞
        with open(original_audio_path, 'rb') as f:
            audio_content = f.read()
        audio_hash = hashlib.sha256(audio_content).hexdigest()
        
        # 1. –°–æ–∑–¥–∞–Ω–∏–µ –∑–∞–ø–∏—Å–∏ –æ –≤—Å—Ç—Ä–µ—á–µ
        meeting = Meeting(
            filename=filename,
            start_time=datetime.now(),
            duration_sec=sum(seg.get('end', 0) - seg.get('start', 0) for seg in segments),
            audio_hash=audio_hash,
            status='completed',
            quality_score=0.95,
            context_tags=json.dumps(["meeting", "transcription"])
        )
        session.add(meeting)
        session.flush()
        
        # 2. –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–ø–∏–∫–µ—Ä–æ–≤
        speaker_cache = {}
        for seg in segments:
            speaker_name = seg.get('speaker', 'SPEAKER_00')
            if speaker_name not in speaker_cache:
                # –ü–æ–∏—Å–∫ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ —Å–ø–∏–∫–µ—Ä–∞
                speaker = session.query(Speaker).filter_by(external_id=speaker_name).first()
                if not speaker:
                    # –°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤–æ–≥–æ —Å–ø–∏–∫–µ—Ä–∞
                    speaker = Speaker(
                        external_id=speaker_name,
                        name=speaker_name,
                        role='–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ'
                    )
                    session.add(speaker)
                    session.flush()
                speaker_cache[speaker_name] = speaker.id
        
        # 3. –°–æ–∑–¥–∞–Ω–∏–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤
        for i, seg in enumerate(segments):
            speaker_name = seg.get('speaker', 'SPEAKER_00')
            speaker_id = speaker_cache[speaker_name]
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤
            technical_terms = extract_technical_terms(seg.get('text', ''))
            fragment = Fragment(
                meeting_id=meeting.id,
                start_time=seg.get('start', 0),
                end_time=seg.get('end', 0),
                speaker_id=speaker_id,
                text=seg.get('text', '').strip(),
                raw_text=seg.get('text', ''),
                importance_score=0.8,
                business_value='–æ–±—Å—É–∂–¥–µ–Ω–∏–µ',
                technical_terms=json.dumps(technical_terms),
                semantic_cluster=i // 5  # –ü—Ä–æ—Å—Ç–∞—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –ø–æ 5 —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤
            )
            session.add(fragment)
            session.flush()
        
        # 4. –ö–æ–º–º–∏—Ç –∏–∑–º–µ–Ω–µ–Ω–∏–π
        session.commit()
        print(f"‚úÖ –î–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ Postgres! –í—Å—Ç—Ä–µ—á–∞ ID: {meeting.id}")
        
        # === –í–°–ï–ì–î–ê —Å–æ—Ö—Ä–∞–Ω—è—Ç—å —Ñ–∞–π–ª—ã –Ω–∞ –¥–∏—Å–∫ (–µ–¥–∏–Ω–∞—è –ª–æ–≥–∏–∫–∞ —Å save_to_file_only) ===
        base_name = Path(filename).stem
        output_path = Path(OUTPUT_DIR)
        output_path.mkdir(exist_ok=True)
        
        # 5. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ Markdown-–æ—Ç—á—ë—Ç–∞
        md_path = output_path / f"{base_name}.md"
        with open(md_path, "w", encoding="utf-8") as f:
            f.write(analysis_md)
        print(f"üìÑ Markdown-–æ—Ç—á—ë—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {md_path}")
        
        # 6. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ JSON —Å –ø–æ–ª–Ω–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–π (–≤–∫–ª—é—á–∞—è —Å—Å—ã–ª–∫—É –Ω–∞ –∑–∞–ø–∏—Å—å –≤ –ë–î)
        json_result = {
            "metadata": {
                "filename": filename,
                "audio_hash": audio_hash,
                "processed_at": datetime.now().isoformat(),
                "duration_sec": meeting.duration_sec,
                "segment_count": len(segments),
                "meeting_id": meeting.id  # ‚Üê –∫—Ä–∏—Ç–∏—á–Ω–æ: —Å–≤—è–∑—å —Å –∑–∞–ø–∏—Å—å—é –≤ –ë–î
            },
            "transcription": [
                {
                    "speaker": seg.get('speaker', 'SPEAKER_00'),
                    "text": seg.get('text', '').strip(),
                    "start": seg.get('start', 0),
                    "end": seg.get('end', 0),
                    "technical_terms": extract_technical_terms(seg.get('text', ''))
                }
                for seg in segments
            ],
            "analysis": {
                "raw_markdown": analysis_md,
                "extracted_terms": list(set(term for seg in segments for term in extract_technical_terms(seg.get('text', ''))))
            }
        }
        json_path = output_path / f"{base_name}.json"
        with open(json_path, "w", encoding="utf-8") as f:
            json.dump(json_result, f, ensure_ascii=False, indent=2)
        print(f"üì¶ JSON-–¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {json_path}")
        
        return str(md_path)
    except Exception as e:
        session.rollback()
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –≤ –±–∞–∑—ã: {e}")
        traceback.print_exc()
        raise
    finally:
        session.close()

# === –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è ===
def main(audio_file: str = None, json_file: str = None, device: str = "cuda", no_db: bool = False):
    # –í–∞–ª–∏–¥–∞—Ü–∏—è –∏—Å—Ç–æ—á–Ω–∏–∫–∞ (—É–∂–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∞ –≤ CLI, –Ω–æ –¥—É–±–ª–∏—Ä—É–µ–º –¥–ª—è –∑–∞—â–∏—Ç—ã)
    if not audio_file and not json_file:
        raise ValueError("–¢—Ä–µ–±—É–µ—Ç—Å—è –∞—É–¥–∏–æ—Ñ–∞–π–ª –ò–õ–ò json_file")
    if audio_file and json_file:
        raise ValueError("–£–∫–∞–∂–∏—Ç–µ –¢–û–õ–¨–ö–û –æ–¥–∏–Ω –∏—Å—Ç–æ—á–Ω–∏–∫")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞
    if device == "cuda" and not torch.cuda.is_available():
        print("‚ö†Ô∏è CUDA –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞, –ø–µ—Ä–µ–∫–ª—é—á–∞—é—Å—å –Ω–∞ CPU")
        device = "cpu"
    
    # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ä–µ–∂–∏–º–∞
    is_reanalyze_mode = json_file is not None
    print(f"\nüöÄ –†–µ–∂–∏–º: {'–ü–µ—Ä–µ–∞–Ω–∞–ª–∏–∑ JSON' if is_reanalyze_mode else '–ü–æ–ª–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞'}")
    if is_reanalyze_mode:
        print("‚è≠Ô∏è  –ó–∞–ø–∏—Å—å –≤ –ë–î –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞ –≤ —Ä–µ–∂–∏–º–µ --json")
    
    try:
        # === –†–µ–∂–∏–º 1: –ü–µ—Ä–µ–∞–Ω–∞–ª–∏–∑ –∏–∑ JSON ===
        if is_reanalyze_mode:
            if not os.path.exists(json_file):
                print(f"‚ùå JSON –Ω–µ –Ω–∞–π–¥–µ–Ω: {json_file}")
                sys.exit(1)
            
            segments, orig_filename, audio_hash, duration = load_segments_from_json(json_file)
            print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(segments)} —Å–µ–≥–º–µ–Ω—Ç–æ–≤ –∏–∑ {orig_filename}")
            
            # –í —Ä–µ–∂–∏–º–µ –ø–µ—Ä–µ–∞–Ω–∞–ª–∏–∑–∞ –ë–î –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç—Å—è ‚Äî –Ω–µ—Ç —Å–≤—è–∑–∏ —Å –∏—Å—Ö–æ–¥–Ω—ã–º –∞—É–¥–∏–æ
            session = qdrant_client = None
        
        # === –†–µ–∂–∏–º 2: –ü–æ–ª–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∞—É–¥–∏–æ ===
        else:
            if not os.path.exists(audio_file):
                print(f"‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {audio_file}")
                sys.exit(1)
            
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ë–î (–µ—Å–ª–∏ —Ä–∞–∑—Ä–µ—à–µ–Ω–∞)
            if not no_db:
                print("üîß –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–∞–∑ –¥–∞–Ω–Ω—ã—Ö...")
                engine = init_db()
                qdrant_client = init_qdrant_client()
                create_collections_if_not_exists(qdrant_client)
                session = get_db_session(engine)
            else:
                print("‚è≠Ô∏è  –ü—Ä–æ–ø—É—Å–∫ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –±–∞–∑ –¥–∞–Ω–Ω—ã—Ö (—Ä–µ–∂–∏–º --no-db)")
                session = qdrant_client = None
            
            # –¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—è
            segments = transcribe_and_diarize(audio_file, device=device)
            orig_filename = os.path.basename(audio_file)
            duration = sum(seg.get('end', 0) - seg.get('start', 0) for seg in segments)
            
            # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Ö–µ—à–∞ (—Ç–æ–ª—å–∫–æ –¥–ª—è –∞—É–¥–∏–æ)
            with open(audio_file, 'rb') as f:
                audio_hash = hashlib.sha256(f.read()).hexdigest()
        
        # === –û–±—â–∏–π —ç—Ç–∞–ø: –∞–Ω–∞–ª–∏–∑ —á–µ—Ä–µ–∑ LLM ===
        print("[DEBUG] –û—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏ –ø–µ—Ä–µ–¥ –∞–Ω–∞–ª–∏–∑–æ–º —á–µ—Ä–µ–∑ LLM...")
        _free_gpu_memory()
        analysis_md = analyze_with_model(segments)
        
        # === –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ ===
        if is_reanalyze_mode or no_db:
            # –†–µ–∂–∏–º –±–µ–∑ –ë–î: —Ç–æ–ª—å–∫–æ —Ñ–∞–π–ª—ã
            md_path = save_to_file_only(orig_filename, segments, analysis_md, audio_hash, duration)
        else:
            # –ü–æ–ª–Ω—ã–π —Ä–µ–∂–∏–º: –ë–î + —Ñ–∞–π–ª—ã
            md_path = save_to_databases(session, qdrant_client, orig_filename, segments, analysis_md, audio_file)
        
        print(f"\nüéâ –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à—ë–Ω! –ü–æ–ª–Ω—ã–π –æ—Ç—á—ë—Ç:\n{md_path}")
        
        # –í—ã–≤–æ–¥ –∫—Ä–∞—Ç–∫–æ–≥–æ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è –≤ –∫–æ–Ω—Å–æ–ª—å
        summary_start = analysis_md.find("### üìù –ö—Ä–∞—Ç–∫–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ")
        if summary_start != -1:
            summary_end = analysis_md.find("###", summary_start + 1)
            if summary_end == -1:
                summary_end = len(analysis_md)
            print("\nüìã –ö–†–ê–¢–ö–û–ï –°–û–î–ï–†–ñ–ê–ù–ò–ï:")
            print(analysis_md[summary_start:summary_end].strip())
    
    except KeyboardInterrupt:
        print("\nüõë –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–µ—Ä–≤–∞–Ω–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
        sys.exit(1)
    except Exception as e:
        print(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
        traceback.print_exc()
        sys.exit(1)
***
***
src/llm/__init__.py
***

***
***
src/llm/adapter.py
***
from typing import Dict, Any, Optional

from src.llm.config import load_models_config, get_active_model_profile
from src.llm.backends.ollama import OllamaBackend
from src.llm.backends.llama_cpp import LlamaCppBackend


class LLMAdapter:
    """
    Infrastructure-level LLM adapter.

    IMPORTANT:
    - Does NOT load model on init
    - Loads backend lazily on first generate()
    """

    def __init__(self, models_config_path: str):
        print("üß† LLMAdapter created (lazy init, no model loaded)")

        self.models_config = load_models_config(models_config_path)
        self.profile = get_active_model_profile(self.models_config)

        self._backend: Optional[Any] = None  # lazy-loaded

    def _init_backend(self):
        """Initialize LLM backend lazily."""
        if self._backend is not None:
            return

        backend_type = self.profile.get("backend")

        if backend_type == "ollama":
            self._backend = OllamaBackend(self.profile)
        elif backend_type == "llama_cpp":
            self._backend = LlamaCppBackend(self.profile)
        else:
            raise ValueError(f"Unsupported backend: {backend_type}")

    def generate(self, prompt: str) -> str:
        # üîë Lazy initialization happens HERE
        self._init_backend()

        params: Dict[str, Any] = self.profile.get("params", {})
        return self._backend.generate(prompt, params)

***
***
src/llm/config.py
***
import os
import yaml
from typing import Dict, Any


class ModelConfigError(Exception):
    pass


def load_models_config(path: str) -> Dict[str, Any]:
    with open(path, "r", encoding="utf-8") as f:
        return yaml.safe_load(f)


def get_active_model_profile(models_config: Dict[str, Any]) -> Dict[str, Any]:
    active_profile = os.getenv("ACTIVE_MODEL_PROFILE")

    if not active_profile:
        active_profile = models_config.get("default_model")

    profiles = models_config.get("profiles", {})

    if active_profile not in profiles:
        raise ModelConfigError(
            f"Model profile '{active_profile}' not found in models.yaml"
        )

    profile = profiles[active_profile]
    profile["profile_name"] = active_profile
    return profile

***
***
src/llm/backends/__init__.py
***

***
***
src/llm/backends/base.py
***
from abc import ABC, abstractmethod
from typing import Dict, Any


class LLMBackend(ABC):
    """
    –ë–∞–∑–æ–≤—ã–π –∫–æ–Ω—Ç—Ä–∞–∫—Ç –¥–ª—è –≤—Å–µ—Ö LLM backend'–æ–≤.
    """

    @abstractmethod
    def generate(self, prompt: str, params: Dict[str, Any]) -> str:
        """
        –í—ã–ø–æ–ª–Ω–∏—Ç—å —Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –∏–Ω—Ñ–µ—Ä–µ–Ω—Å.

        :param prompt: –∏—Ç–æ–≥–æ–≤—ã–π prompt
        :param params: –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ (temperature, max_tokens, etc)
        :return: —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
        """
        raise NotImplementedError

***
***
src/llm/backends/llama_cpp.py
***
from typing import Dict, Any

from llama_cpp import Llama

from src.llm.backends.base import LLMBackend


class LlamaCppBackend(LLMBackend):
    """
    Backend –¥–ª—è llama.cpp.
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å –æ–¥–∏–Ω —Ä–∞–∑ –∏ –≤—ã–ø–æ–ª–Ω—è–µ—Ç —Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –∏–Ω—Ñ–µ—Ä–µ–Ω—Å.
    """

    def __init__(self, profile: Dict[str, Any]):
        self.profile = profile

        model_path = profile.get("path")
        if not model_path:
            raise ValueError("llama_cpp backend requires 'path' in profile")

        params = profile.get("params", {})

        # --- –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ (–û–î–ò–ù –†–ê–ó) ---
        self.llm = Llama(
            model_path=model_path,
            n_ctx=params.get("n_ctx", 4096),
            n_gpu_layers=params.get("n_gpu_layers", 0),
            n_batch=params.get("n_batch", 512),
            verbose=params.get("verbose", False),
        )

        # --- –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é ---
        self.default_generation_params: Dict[str, Any] = {
            "temperature": params.get("temperature", 0.1),
            "top_p": params.get("top_p", 0.9),
            "repeat_penalty": params.get("repeat_penalty", 1.1),
            "max_tokens": params.get("max_tokens", 2048),
        }

    def generate(self, prompt: str, params: Dict[str, Any] | None = None) -> str:
        """
        –í—ã–ø–æ–ª–Ω–∏—Ç—å —Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –∏–Ω—Ñ–µ—Ä–µ–Ω—Å.

        :param prompt: –≥–æ—Ç–æ–≤—ã–π prompt
        :param params: –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ (–ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª—è—é—Ç defaults)
        :return: —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
        """

        if params is None:
            params = {}

        generation_params = {
            **self.default_generation_params,
            **params,
        }

        result = self.llm(
            prompt,
            temperature=generation_params["temperature"],
            top_p=generation_params["top_p"],
            repeat_penalty=generation_params["repeat_penalty"],
            max_tokens=generation_params["max_tokens"],
            echo=False,
        )

        # llama_cpp –≤—Å–µ–≥–¥–∞ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç choices
        return result["choices"][0]["text"].strip()

***
***
src/llm/backends/ollama.py
***
from typing import Dict, Any
from src.llm.backends.base import LLMBackend


class OllamaBackend(LLMBackend):
    def __init__(self, profile: Dict[str, Any]):
        self.profile = profile
        self.model_name = profile.get("name")

    def generate(self, prompt: str, params: Dict[str, Any]) -> str:
        raise NotImplementedError("OllamaBackend not implemented yet")

***
***
src/pipeline_v2/context.py
***
from dataclasses import dataclass, field
from pathlib import Path
from uuid import UUID
from typing import Dict, Any
import hashlib

from src.pipeline_v2.services import Services


@dataclass
class PipelineContext:
    """
    Stateless pipeline context.
    Used ONLY to pass data between steps.
    """

    job_id: UUID
    source_type: str           # audio | json
    source_path: Path
    services: Services         # üîë DI container

    source_hash: str = field(init=False)

    # Data-plane artifacts
    artifacts: Dict[str, Any] = field(default_factory=dict)

    def __post_init__(self):
        if self.source_type == "audio":
            with open(self.source_path, "rb") as f:
                self.source_hash = hashlib.sha256(f.read()).hexdigest()
        else:
            self.source_hash = hashlib.sha256(
                str(self.source_path).encode()
            ).hexdigest()

***
***
src/pipeline_v2/orchestrator.py
***
from src.jobs.job_step_repository import JobStepRepository
from src.jobs.models import StepStatus

from src.pipeline_v2.steps.transcription import TranscriptionStep
from src.pipeline_v2.steps.analysis import AnalysisStep


class PipelineOrchestrator:
    """
    Orchestrates execution of pipeline steps.
    Control-plane state is stored ONLY in job_steps table.
    """

    def __init__(self):
        self.repo = JobStepRepository()

        # –ø–æ—Ä—è–¥–æ–∫ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è pipeline
        self.steps = [
            TranscriptionStep(),
            AnalysisStep(),
        ]

    def run(self, job, ctx):
        """
        Execute pipeline for a given job and context.
        """

        for step in self.steps:
            # 1. –ü–æ–ª—É—á–∞–µ–º –∏–ª–∏ —Å–æ–∑–¥–∞—ë–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ —à–∞–≥–∞
            step_state = self.repo.create_if_not_exists(
                job_id=job.id,
                step_name=step.name,
            )

            # 2. –ï—Å–ª–∏ —à–∞–≥ —É–∂–µ –∑–∞–≤–µ—Ä—à—ë–Ω ‚Äî –ø—Ä–æ—Å—Ç–æ –ø—Ä–æ–∫–∏–¥—ã–≤–∞–µ–º artifacts –¥–∞–ª—å—à–µ
            if step_state.status == StepStatus.COMPLETED:
                if step_state.artifacts:
                    ctx.artifacts[step.name] = step_state.artifacts
                continue

            # 3. RUNNING ‚Äî —ç—Ç–æ –∏–Ω–≤–∞—Ä–∏–∞–Ω—Ç–Ω–∞—è –æ—à–∏–±–∫–∞
            if step_state.status == StepStatus.RUNNING:
                raise RuntimeError(
                    f"Step '{step.name}' already RUNNING for job {job.id}"
                )

            # 4. –ü–µ—Ä–µ–≤–æ–¥–∏–º –≤ RUNNING (attempt++)
            self.repo.mark_running(step_state)

            try:
                # 5. –í—ã–ø–æ–ª–Ω—è–µ–º —à–∞–≥
                result = step.run(ctx)

                # 6. –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
                if result.status == "completed":
                    self.repo.mark_completed(
                        step_state,
                        artifacts=result.artifacts,
                    )

                    # üîë –ö–õ–Æ–ß–ï–í–û–ï –ú–ï–°–¢–û
                    # –ø–µ—Ä–µ–¥–∞—ë–º artifacts —Å–ª–µ–¥—É—é—â–µ–º—É —à–∞–≥—É
                    ctx.artifacts[step.name] = result.artifacts or {}

                else:
                    self.repo.mark_failed(
                        step_state,
                        error=result.error or "unknown error",
                    )
                    return  # pipeline –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç—Å—è

            except Exception as e:
                # 7. –õ—é–±–æ–µ –∏—Å–∫–ª—é—á–µ–Ω–∏–µ = FAILED
                self.repo.mark_failed(step_state, str(e))
                raise

***
***
src/pipeline_v2/services.py
***
# src/pipeline_v2/services.py

from dataclasses import dataclass
from src.llm.adapter import LLMAdapter


@dataclass
class Services:
    """
    Infrastructure services container.
    Lives on Worker level, injected into PipelineContext.
    """
    llm: LLMAdapter

***
***
src/pipeline_v2/steps/__init__.py
***

***
***
src/pipeline_v2/steps/analysis.py
***
import json
from pathlib import Path
from datetime import datetime

from src.pipeline_v2.steps.base import Step, StepResult
from src.prompts.registry import PromptRegistry
from src.contracts.analysis_result import AnalysisResult


class AnalysisStep(Step):
    name = "analysis"

    PROMPT_PATH = "analysis/v1.yaml"

    def __init__(self):
        self.prompt_registry = PromptRegistry()

    def run(self, ctx) -> StepResult:
        # 1. –ü–æ–ª—É—á–∞–µ–º transcription artifacts
        transcription = ctx.artifacts.get("transcription")
        if not transcription:
            return StepResult(
                status="failed",
                error="Missing transcription artifacts",
            )

        segments_path = transcription.get("segments_path")
        if not segments_path:
            return StepResult(
                status="failed",
                error="Missing segments_path in transcription artifacts",
            )

        segments_path = Path(segments_path)
        if not segments_path.exists():
            return StepResult(
                status="failed",
                error=f"Segments file not found: {segments_path}",
            )

        # 2. –ó–∞–≥—Ä—É–∂–∞–µ–º —Å–µ–≥–º–µ–Ω—Ç—ã
        with open(segments_path, "r", encoding="utf-8") as f:
            segments = json.load(f)

        transcript = "\n".join(seg.get("text", "") for seg in segments).strip()
        if not transcript:
            return StepResult(
                status="failed",
                error="Empty transcription text",
            )

        # 3. Render prompt
        prompt = self.prompt_registry.render(
            self.PROMPT_PATH,
            transcript=transcript,
        )

        # 4. LLM inference
        try:
            llm_response = ctx.services.llm.generate(prompt)
        except Exception as e:
            return StepResult(
                status="failed",
                error=f"LLM inference failed: {e}",
            )

        # 5. Build result contract
        result = AnalysisResult(
            prompt_id="analysis.v1",
            generated_at=datetime.utcnow(),
            summary_raw=llm_response,
            segment_count=len(segments),
            model_backend=ctx.services.llm.profile.get("backend"),
            model_profile=ctx.services.llm.profile.get("profile_name"),
        )

        # 6. Persist
        output_path = segments_path.with_name(
            segments_path.stem.replace("_segments", "_analysis") + ".json"
        )

        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(result.__dict__, f, ensure_ascii=False, indent=2, default=str)

        return StepResult(
            status="completed",
            artifacts={
                "analysis_path": str(output_path),
                "prompt_id": result.prompt_id,
                "segment_count": result.segment_count,
            },
        )

***
***
src/pipeline_v2/steps/base.py
***
from dataclasses import dataclass
from typing import Optional, Dict, Any, Literal


@dataclass
class StepResult:
    status: Literal["completed", "failed"]
    artifacts: Optional[Dict[str, Any]] = None
    error: Optional[str] = None


class Step:
    """
    Stateless pipeline step.
    MUST NOT store or check execution state.
    """

    name: str

    def run(self, ctx) -> StepResult:
        raise NotImplementedError

***
***
src/pipeline_v2/steps/transcription.py
***
from pathlib import Path
import json

from src.pipeline_v2.steps.base import Step, StepResult
from src.legacy.pipeline.main import transcribe_and_diarize


class TranscriptionStep(Step):
    name = "transcription"

    def run(self, ctx):
        segments = transcribe_and_diarize(
            audio_path=str(ctx.source_path)
        )

        out_dir = Path("output")
        out_dir.mkdir(exist_ok=True)

        segments_path = out_dir / f"{ctx.source_hash}_segments.json"
        segments_path.write_text(
            json.dumps(segments, ensure_ascii=False, indent=2),
            encoding="utf-8",
        )

        return StepResult(
            status="completed",
            artifacts={
                "segments_path": str(segments_path),
                "segment_count": len(segments),
            },
        )

***
***
src/legacy/__init__.py
***

***
***
src/legacy/cli.py
***
#!/usr/bin/env python3
"""CLI –¥–ª—è pipeline –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤—Å—Ç—Ä–µ—á."""
import argparse
from pathlib import Path
from src.legacy.pipeline.main import main as run_pipeline

def main():
    parser = argparse.ArgumentParser(description="Pipeline –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –≤—Å—Ç—Ä–µ—á")
    parser.add_argument("audio_file", nargs="?", type=Path, 
                        help="–ü—É—Ç—å –∫ –∞—É–¥–∏–æ—Ñ–∞–π–ª—É (–æ–±—è–∑–∞—Ç–µ–ª–µ–Ω, –µ—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω --json)")
    parser.add_argument("--json", type=Path, 
                        help="–ü—É—Ç—å –∫ JSON –¥–ª—è –ø–µ—Ä–µ–∞–Ω–∞–ª–∏–∑–∞ (–ø—Ä–æ–ø—É—Å–∫–∞–µ—Ç —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—é –∏ –∑–∞–ø–∏—Å—å –≤ –ë–î)")
    parser.add_argument("--device", choices=["cuda", "cpu"], default="cuda")
    parser.add_argument("--no-db", action="store_true", 
                        help="–ë–µ–∑ –∑–∞–ø–∏—Å–∏ –≤ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö (—Ç–æ–ª—å–∫–æ —Ñ–∞–π–ª—ã). –ò–≥–Ω–æ—Ä–∏—Ä—É–µ—Ç—Å—è –ø—Ä–∏ --json")

    args = parser.parse_args()

    # –í–∞–ª–∏–¥–∞—Ü–∏—è: —Ä–æ–≤–Ω–æ –æ–¥–∏–Ω –∏—Å—Ç–æ—á–Ω–∏–∫
    if not args.audio_file and not args.json:
        parser.error("–£–∫–∞–∂–∏—Ç–µ –∞—É–¥–∏–æ—Ñ–∞–π–ª –ò–õ–ò --json")
    if args.audio_file and args.json:
        parser.error("–£–∫–∞–∂–∏—Ç–µ –¢–û–õ–¨–ö–û –æ–¥–∏–Ω –∏—Å—Ç–æ—á–Ω–∏–∫: –∞—É–¥–∏–æ –ò–õ–ò JSON")

    # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –≤–∫–ª—é—á–µ–Ω–∏–µ --no-db –ø—Ä–∏ --json (–∑–∞–ø–∏—Å—å –≤ –ë–î –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞)
    effective_no_db = True if args.json else args.no_db

    # –ó–∞–ø—É—Å–∫ –ø–∞–π–ø–ª–∞–π–Ω–∞
    run_pipeline(
        audio_file=str(args.audio_file) if args.audio_file else None,
        json_file=str(args.json) if args.json else None,
        device=args.device,
        no_db=effective_no_db
    )

if __name__ == "__main__":
    main()
***
***
src/legacy/config/__init__.py
***

***
***
src/legacy/config/models.py
***
"""
–ó–∞–≥—Ä—É–∑—á–∏–∫ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –º–æ–¥–µ–ª–µ–π –∏–∑ models.yaml
"""
import os
import yaml
from pathlib import Path
from typing import Dict, Any, Optional
from dotenv import load_dotenv

load_dotenv()

class ModelConfigError(Exception):
    """–û—à–∏–±–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –º–æ–¥–µ–ª–∏"""
    pass

class ModelProfile:
    """–ü—Ä–æ—Ñ–∏–ª—å –º–æ–¥–µ–ª–∏ —Å –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π"""
    def __init__(self, key: str, config: Dict[str, Any]):
        self.key = key
        self.backend = config.get('backend')
        self.name = config.get('name')
        self.path = config.get('path')
        self.description = config.get('description', '')
        self.params = config.get('params', {})
        
        # –í–∞–ª–∏–¥–∞—Ü–∏—è –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –ø–æ–ª–µ–π
        if not self.backend:
            raise ModelConfigError(f"–ü—Ä–æ—Ñ–∏–ª—å '{key}': –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –ø–æ–ª–µ 'backend'")
        
        if self.backend == 'ollama' and not self.name:
            raise ModelConfigError(f"–ü—Ä–æ—Ñ–∏–ª—å '{key}': –¥–ª—è backend='ollama' —Ç—Ä–µ–±—É–µ—Ç—Å—è –ø–æ–ª–µ 'name'")
        if self.backend == 'llama_cpp' and not self.path:
            raise ModelConfigError(f"–ü—Ä–æ—Ñ–∏–ª—å '{key}': –¥–ª—è backend='llama_cpp' —Ç—Ä–µ–±—É–µ—Ç—Å—è –ø–æ–ª–µ 'path'")

    def __repr__(self):
        return f"<ModelProfile key={self.key} backend={self.backend}>"

class ModelsConfig:
    """–ö–æ–Ω—Ç–µ–π–Ω–µ—Ä –¥–ª—è –≤—Å–µ—Ö –ø—Ä–æ—Ñ–∏–ª–µ–π –º–æ–¥–µ–ª–µ–π"""
    def __init__(self, config_path: str = "models.yaml"):
        self.config_path = Path(config_path)
        self.profiles: Dict[str, ModelProfile] = {}
        self.default_model: str = ""
        self._load()
    
    def _load(self):
        if not self.config_path.exists():
            raise ModelConfigError(f"–§–∞–π–ª –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω: {self.config_path}")
        
        with open(self.config_path, 'r', encoding='utf-8') as f:
            raw_config = yaml.safe_load(f)
        
        self.default_model = raw_config.get('default_model', '')
        profiles_raw = raw_config.get('profiles', {})
        
        if not profiles_raw:
            raise ModelConfigError("–í –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –ø—Ä–æ—Ñ–∏–ª–∏ –º–æ–¥–µ–ª–µ–π")
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –≤—Å–µ—Ö –ø—Ä–æ—Ñ–∏–ª–µ–π
        for key, cfg in profiles_raw.items():
            try:
                self.profiles[key] = ModelProfile(key, cfg)
            except ModelConfigError as e:
                print(f"‚ö†Ô∏è –ü—Ä–æ–ø—É—â–µ–Ω –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π –ø—Ä–æ—Ñ–∏–ª—å '{key}': {e}")
    
    def get_active_profile(self) -> ModelProfile:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—Ä–æ—Ñ–∏–ª—å, —É–∫–∞–∑–∞–Ω–Ω—ã–π –≤ ACTIVE_MODEL_PROFILE –∏–ª–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é"""
        active_key = os.getenv('ACTIVE_MODEL_PROFILE', self.default_model)
        
        if not active_key:
            raise ModelConfigError(
                "–ù–µ —É–∫–∞–∑–∞–Ω–∞ –∞–∫—Ç–∏–≤–Ω–∞—è –º–æ–¥–µ–ª—å. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ ACTIVE_MODEL_PROFILE –≤ .env "
                "–∏–ª–∏ –∑–∞–¥–∞–π—Ç–µ default_model –≤ models.yaml"
            )
        
        profile = self.profiles.get(active_key)
        if not profile:
            available = ', '.join(self.profiles.keys())
            raise ModelConfigError(
                f"–ü—Ä–æ—Ñ–∏–ª—å '{active_key}' –Ω–µ –Ω–∞–π–¥–µ–Ω. –î–æ—Å—Ç—É–ø–Ω—ã–µ –ø—Ä–æ—Ñ–∏–ª–∏: {available}"
            )
        
        return profile
    
    def list_profiles(self) -> Dict[str, str]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å {–∫–ª—é—á: –æ–ø–∏—Å–∞–Ω–∏–µ} –¥–ª—è –≤—Å–µ—Ö –ø—Ä–æ—Ñ–∏–ª–µ–π"""
        return {key: profile.description for key, profile in self.profiles.items()}

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä –¥–ª—è –∏–º–ø–æ—Ä—Ç–∞
_models_config: Optional[ModelsConfig] = None

def get_models_config() -> ModelsConfig:
    """–õ–µ–Ω–∏–≤–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"""
    global _models_config
    if _models_config is None:
        _models_config = ModelsConfig()
    return _models_config
***
***
src/legacy/pipeline/__init__.py
***

***
***
src/legacy/pipeline/main.py
***
#!/usr/bin/env python3
"""
–ï–¥–∏–Ω—ã–π legacy-pipeline –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤—Å—Ç—Ä–µ—á.

–í–ê–ñ–ù–û:
- WhisperX –≤—ã–∑—ã–≤–∞–µ—Ç—Å—è –¢–û–õ–¨–ö–û –µ—Å–ª–∏ –ù–ï –ø–µ—Ä–µ–¥–∞–Ω precomputed_segments_path
- sys.exit() –ó–ê–ü–†–ï–©–Å–ù ‚Äî —Ç–æ–ª—å–∫–æ –∏—Å–∫–ª—é—á–µ–Ω–∏—è
"""

import os
import json
import gc
import sys
import hashlib
import traceback
from pathlib import Path
from datetime import datetime

import torch
import ollama
from dotenv import load_dotenv

# =========================================================
# CONFIG
# =========================================================

load_dotenv()

INPUT_DIR = os.getenv("INPUT_DIR", "input")
OUTPUT_DIR = os.getenv("OUTPUT_DIR", "output")
HF_TOKEN = os.getenv("HF_TOKEN")

Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)

# =========================================================
# STORAGE / AI IMPORTS
# =========================================================

from src.legacy.storage.postgres import (
    init_db,
    get_db_session,
    Meeting,
    Speaker,
    Fragment,
)
from src.legacy.storage.qdrant import (
    init_qdrant_client,
    create_collections_if_not_exists,
)
from src.legacy.config.models import get_models_config
from src.legacy.ai.generator import generate_text

# =========================================================
# UTILS
# =========================================================

def _free_gpu_memory():
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.synchronize()
    gc.collect()


# =========================================================
# LOAD SEGMENTS FROM JSON
# =========================================================

def load_segments_from_json(json_path: str):
    with open(json_path, "r", encoding="utf-8") as f:
        data = json.load(f)

    if "transcription" not in data:
        raise ValueError("JSON –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç transcription")

    segments = [
        {
            "speaker": seg["speaker"],
            "text": seg["text"],
            "start": seg["start"],
            "end": seg["end"],
        }
        for seg in data["transcription"]
    ]

    meta = data.get("metadata", {})
    filename = meta.get("filename", Path(json_path).stem)
    audio_hash = meta.get("audio_hash", "")
    duration = meta.get(
        "duration_sec",
        sum(seg["end"] - seg["start"] for seg in segments),
    )

    return segments, filename, audio_hash, duration


# =========================================================
# WHISPERX (–ï–î–ò–ù–°–¢–í–ï–ù–ù–û–ï –ú–ï–°–¢–û)
# =========================================================

def transcribe_and_diarize(audio_path: str, device: str = "cuda"):
    import whisperx
    from pydub import AudioSegment

    print(f"[DEBUG] WhisperX load (device={device})")

    model = whisperx.load_model(
        "large-v3",
        device,
        compute_type="float16" if device == "cuda" else "int8",
    )

    audio = AudioSegment.from_file(audio_path)
    audio = audio.set_channels(1).set_frame_rate(16000)

    wav_path = "temp_audio.wav"
    audio.export(wav_path, format="wav")

    audio_data = whisperx.load_audio(wav_path)
    result = model.transcribe(audio_data, language="ru")

    model_a, metadata = whisperx.load_align_model(
        language_code="ru",
        device=device,
        model_name="facebook/wav2vec2-base-960h",
    )

    result = whisperx.align(
        result["segments"], model_a, metadata, audio_data, device
    )

    del model_a
    _free_gpu_memory()

    diarize_model = whisperx.DiarizationPipeline(
        use_auth_token=HF_TOKEN,
        device=device,
    )
    diarized = diarize_model(audio_data)
    result = whisperx.assign_word_speakers(diarized, result)

    segments = [
        {
            "speaker": seg.get("speaker", "SPEAKER_00"),
            "text": seg.get("text", "").strip(),
            "start": round(seg.get("start", 0), 2),
            "end": round(seg.get("end", 0), 2),
        }
        for seg in result["segments"]
    ]

    del model, diarize_model, audio_data, diarized, result
    _free_gpu_memory()

    if os.path.exists(wav_path):
        os.remove(wav_path)

    print(f"[DEBUG] WhisperX done: {len(segments)} segments")
    return segments


# =========================================================
# ANALYSIS
# =========================================================

def analyze_with_model(segments: list) -> str:
    cfg = get_models_config()
    profile = cfg.get_active_profile()

    dialogue = "\n".join(
        f"{s['speaker']}: {s['text']}" for s in segments if s["text"]
    )

    system_prompt = f"""
–¢—ã ‚Äî —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∞–Ω–∞–ª–∏–∑—É —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –≤—Å—Ç—Ä–µ—á –∫–æ–º–ø–∞–Ω–∏–∏ –°–æ–º–º–µ—Ä—Å.
–ü–æ–¥–≥–æ—Ç–æ–≤—å –°–¢–†–£–ö–¢–£–†–ò–†–û–í–ê–ù–ù–´–ô Markdown-–æ—Ç—á—ë—Ç.

–î–∞—Ç–∞: {datetime.now().strftime('%d.%m.%Y')}
"""

    if profile.backend == "ollama":
        response = ollama.chat(
            model=profile.name,
            messages=[
                {"role": "system", "content": system_prompt.strip()},
                {"role": "user", "content": dialogue},
            ],
            stream=False,
        )
        return response["message"]["content"].strip()

    return generate_text(
        f"{system_prompt}\n\n{dialogue}",
        profile,
    )


# =========================================================
# SAVE FILES ONLY
# =========================================================

def save_to_file_only(
    filename: str,
    segments: list,
    analysis_md: str,
    audio_hash: str,
    duration: float,
):
    base = Path(filename).stem
    out = Path(OUTPUT_DIR)

    md_path = out / f"{base}.md"
    json_path = out / f"{base}.json"

    md_path.write_text(analysis_md, encoding="utf-8")

    json.dump(
        {
            "metadata": {
                "filename": filename,
                "audio_hash": audio_hash,
                "duration_sec": duration,
                "processed_at": datetime.now().isoformat(),
            },
            "transcription": segments,
            "analysis": {"raw_markdown": analysis_md},
        },
        open(json_path, "w", encoding="utf-8"),
        ensure_ascii=False,
        indent=2,
    )

    return str(md_path)


# =========================================================
# SAVE TO DATABASES
# =========================================================

def save_to_databases(session, qdrant_client, filename, segments, analysis_md, original_audio_path):
    """–ò–¥–µ–º–ø–æ—Ç–µ–Ω—Ç–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ Postgres –∏ —Ñ–∞–π–ª—ã"""
    print("üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –≤ –±–∞–∑—ã...")

    try:
        # --- hash ---
        with open(original_audio_path, "rb") as f:
            audio_hash = hashlib.sha256(f.read()).hexdigest()

        # --- –∏—â–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é –≤—Å—Ç—Ä–µ—á—É ---
        meeting = (
            session.query(Meeting)
            .filter(Meeting.filename == filename)
            .one_or_none()
        )

        if meeting:
            print(f"‚ôªÔ∏è –í—Å—Ç—Ä–µ—á–∞ —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç (id={meeting.id}), –æ–±–Ω–æ–≤–ª—è–µ–º")
            meeting.updated_at = datetime.now()
            meeting.status = "completed"

            # —á–∏—Å—Ç–∏–º —Å—Ç–∞—Ä—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã
            session.query(Fragment).filter(
                Fragment.meeting_id == meeting.id
            ).delete()

        else:
            print("üÜï –°–æ–∑–¥–∞—ë–º –Ω–æ–≤—É—é –≤—Å—Ç—Ä–µ—á—É")
            meeting = Meeting(
                filename=filename,
                start_time=datetime.now(),
                duration_sec=0,
                audio_hash=audio_hash,
                status="completed",
                quality_score=0.95,
                context_tags="[]",
            )
            session.add(meeting)
            session.flush()

        # --- —Å–ø–∏–∫–µ—Ä—ã ---
        speaker_cache = {}
        for seg in segments:
            name = seg.get("speaker", "SPEAKER_00")
            if name not in speaker_cache:
                speaker = (
                    session.query(Speaker)
                    .filter_by(external_id=name)
                    .one_or_none()
                )
                if not speaker:
                    speaker = Speaker(
                        external_id=name,
                        name=name,
                        role="unknown",
                    )
                    session.add(speaker)
                    session.flush()
                speaker_cache[name] = speaker.id

        # --- —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã ---
        duration = 0.0
        for i, seg in enumerate(segments):
            duration += seg["end"] - seg["start"]
            fragment = Fragment(
                meeting_id=meeting.id,
                start_time=seg["start"],
                end_time=seg["end"],
                speaker_id=speaker_cache[seg["speaker"]],
                text=seg["text"],
                raw_text=seg["text"],
                importance_score=0.8,
                business_value="discussion",
                technical_terms=json.dumps(
                    extract_technical_terms(seg["text"])
                ),
                semantic_cluster=i // 5,
            )
            session.add(fragment)

        meeting.duration_sec = duration
        session.commit()

        print(f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤ Postgres (meeting_id={meeting.id})")

        # --- —Ñ–∞–π–ª—ã ---
        base = Path(filename).stem
        out = Path(OUTPUT_DIR)
        out.mkdir(exist_ok=True)

        md_path = out / f"{base}.md"
        md_path.write_text(analysis_md, encoding="utf-8")

        json_path = out / f"{base}.json"
        json_path.write_text(
            json.dumps(
                {
                    "metadata": {
                        "filename": filename,
                        "audio_hash": audio_hash,
                        "meeting_id": meeting.id,
                        "duration_sec": duration,
                    },
                    "transcription": segments,
                    "analysis": analysis_md,
                },
                ensure_ascii=False,
                indent=2,
            ),
            encoding="utf-8",
        )

        return str(md_path)

    except Exception:
        session.rollback()
        raise

    finally:
        session.close()



# =========================================================
# MAIN
# =========================================================

def main(
    audio_file: str = None,
    json_file: str = None,
    device: str = "cuda",
    no_db: bool = False,
    precomputed_segments_path: str | None = None,
):
    if not audio_file and not json_file:
        raise ValueError("–¢—Ä–µ–±—É–µ—Ç—Å—è –∞—É–¥–∏–æ—Ñ–∞–π–ª –ò–õ–ò json_file")

    if audio_file and json_file:
        raise ValueError("–£–∫–∞–∂–∏—Ç–µ –¢–û–õ–¨–ö–û –æ–¥–∏–Ω –∏—Å—Ç–æ—á–Ω–∏–∫")

    if device == "cuda" and not torch.cuda.is_available():
        device = "cpu"

    is_reanalyze_mode = json_file is not None
    print(f"\nüöÄ –†–µ–∂–∏–º: {'–ü–µ—Ä–µ–∞–Ω–∞–ª–∏–∑ JSON' if is_reanalyze_mode else '–ü–æ–ª–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞'}")

    try:
        # --- JSON ---
        if is_reanalyze_mode:
            segments, orig_filename, audio_hash, duration = load_segments_from_json(json_file)
            session = None
            qdrant_client = None

        # --- AUDIO ---
        else:
            if not precomputed_segments_path:
                raise RuntimeError(
                    "Whisper –∑–∞–ø—Ä–µ—â—ë–Ω –≤ legacy pipeline. "
                    "–ò—Å–ø–æ–ª—å–∑—É–π TranscriptionStep."
                )

            print(f"[DEBUG] –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–µ–¥—Ä–∞—Å—á–∏—Ç–∞–Ω–Ω—ã–µ —Å–µ–≥–º–µ–Ω—Ç—ã: {precomputed_segments_path}")
            with open(precomputed_segments_path, "r", encoding="utf-8") as f:
                segments = json.load(f)

            orig_filename = os.path.basename(audio_file)
            duration = sum(seg.get("end", 0) - seg.get("start", 0) for seg in segments)

            with open(audio_file, "rb") as f:
                audio_hash = hashlib.sha256(f.read()).hexdigest()

            if not no_db:
                engine = init_db()
                qdrant_client = init_qdrant_client()
                create_collections_if_not_exists(qdrant_client)
                session = get_db_session(engine)
            else:
                session = None
                qdrant_client = None

        _free_gpu_memory()
        analysis_md = analyze_with_model(segments)

        if is_reanalyze_mode or no_db:
            md_path = save_to_file_only(
                orig_filename,
                segments,
                analysis_md,
                audio_hash,
                duration,
            )
        else:
            md_path = save_to_databases(
                session,
                qdrant_client,
                orig_filename,
                segments,
                analysis_md,
                audio_file,
            )

        print(f"\nüéâ –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à—ë–Ω:\n{md_path}")

    except Exception:
        traceback.print_exc()
        raise


***
***
src/legacy/utils/__init__.py
***

***
***
src/legacy/utils/term_loader.py
***
from qdrant_client import QdrantClient
from qdrant_client.http import models
from sentence_transformers import SentenceTransformer
import uuid

client = QdrantClient(host='localhost', port=6333)
encoder = SentenceTransformer('all-MiniLM-L6-v2')

term_data = {
    'term_id': 'tms',
    'term_name': 'TMS',
    'full_name': 'Terminal Management System',
    'definition': '–ü—Ä–æ–≥—Ä–∞–º–º–Ω–æ–µ –æ–±–µ—Å–ø–µ—á–µ–Ω–∏–µ –¥–ª—è —Ü–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–≥–æ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ø–∞—Ä–∫–æ–º POS-—Ç–µ—Ä–º–∏–Ω–∞–ª–æ–≤. –û–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç —É–¥–∞–ª–µ–Ω–Ω—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é, –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ —Å–æ—Å—Ç–æ—è–Ω–∏—è, –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø—Ä–æ—à–∏–≤–æ–∫ –∏ —Å–±–æ—Ä –¥–∏–∞–≥–Ω–æ—Å—Ç–∏—á–µ—Å–∫–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ —Å —Ç–µ—Ä–º–∏–Ω–∞–ª–æ–≤.',
    'related_terms': ['–ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö', 'POS-—Ç–µ—Ä–º–∏–Ω–∞–ª', '–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è', '–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥', '–õ–∏—Ü–µ–Ω–∑–∏—Ä–æ–≤–∞–Ω–∏–µ', 'Somers.POS'],
    'business_context': ['—É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Å–µ—Ç—å—é —Ç–µ—Ä–º–∏–Ω–∞–ª–æ–≤', '–†–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏–µ –Ω–æ–≤—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π –Ω–∞ —Ç–µ—Ä–º–∏–Ω–∞–ª–∞—Ö', '–î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –ø—Ä–æ–±–ª–µ–º —Å —Ç–µ—Ä–º–∏–Ω–∞–ª–∞–º–∏', '–û–±–µ—Å–ø–µ—á–µ–Ω–∏–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º –ù–°–ü–ö'],
    'regulatory_info': '–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º —Ä–µ–≥—É–ª—è—Ç–æ—Ä–æ–≤ –ø–æ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø–ª–∞—Ç–µ–∂–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏ —É–¥–∞–ª–µ–Ω–Ω–æ–º—É —É–ø—Ä–∞–≤–ª–µ–Ω–∏—é —Ç–µ—Ä–º–∏–Ω–∞–ª–∞–º–∏.',
    'common_misconceptions': [],
    'examples': [
        '–¢–µ—Ä–º–∏–Ω–∞–ª —Å—Ö–æ–¥–∏–ª –Ω–∞ TMS –∏ –ø–æ–ª—É—á–∏–ª –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é.',
        '–¢–µ—Ä–º–∏–Ω–∞–ª –∏—Å–ø–æ–ª—å–∑—É–µ—Ç TMS, —á—Ç–æ-–±—ã –æ–±–Ω–æ–≤–ª—è—Ç—å —Å–≤–æ–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã.',
        '–ù–∞ TMS –º–æ–∂–Ω–æ –æ—Ç—Å–ª–µ–∂–∏–≤–∞—Ç—å —Ä–∞–±–æ—Ç—É —Ç–µ—Ä–º–∏–Ω–∞–ª–æ–≤.',
        '–ò–Ω–∂–µ–Ω–µ—Ä –°–æ–º–µ—Ä—Å –∑–∞—Ö–æ–¥–∏—Ç –≤ TMS –¥–ª—è –≤—ã—Å—Ç–∞–≤–ª–µ–Ω–∏—è –Ω–∞—Å—Ç—Ä–æ–µ–∫ –°–ë–ü –Ω–∞ –Ω–æ–≤—ã–µ —Ç–µ—Ä–º–∏–Ω–∞–ª—ã'
    ],
    'importance_level': 9,
    'last_updated': '2026-01-14'
}

embedding = encoder.encode(term_data['definition']).tolist()
# –ò—Å–ø–æ–ª—å–∑—É–µ–º UUID –≤–º–µ—Å—Ç–æ —Å—Ç—Ä–æ–∫–∏
point_id = str(uuid.uuid4())

client.upsert(
    collection_name='technical_terms',
    points=[models.PointStruct(id=point_id, vector=embedding, payload=term_data)]
)
print(f'‚úÖ TMS –¥–æ–±–∞–≤–ª–µ–Ω –≤ Qdrant! ID: {point_id}')

***
***
src/legacy/storage/__init__.py
***

***
***
src/legacy/storage/postgres.py
***
import os
from sqlalchemy import create_engine, Column, Integer, String, Float, DateTime, Boolean, Text, JSON, ForeignKey, func
from sqlalchemy import text
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker, relationship
from sqlalchemy.dialects.postgresql import UUID
from datetime import datetime, timezone
import uuid
from dotenv import load_dotenv

load_dotenv()

Base = declarative_base()

class Meeting(Base):
    __tablename__ = 'meetings'
    
    id = Column(Integer, primary_key=True)
    filename = Column(String, unique=True, nullable=False)
    start_time = Column(DateTime(timezone=True), nullable=False, default=func.now())
    duration_sec = Column(Integer, nullable=False)
    audio_hash = Column(String(64), unique=True, nullable=False)
    processed_at = Column(DateTime(timezone=True), default=func.now())
    status = Column(String(20), default='raw')
    quality_score = Column(Float)
    context_tags = Column(JSON, default=[])
    created_at = Column(DateTime(timezone=True), default=func.now())
    updated_at = Column(DateTime(timezone=True), default=func.now(), onupdate=func.now())

class Speaker(Base):
    __tablename__ = 'speakers'
    
    id = Column(Integer, primary_key=True)
    external_id = Column(String(50), unique=True)
    name = Column(String(100))
    role = Column(String(50))
    voice_embedding = Column(JSON)
    confidence_score = Column(Float)
    last_confirmed = Column(DateTime(timezone=True))
    is_verified = Column(Boolean, default=False)
    voice_samples_count = Column(Integer, default=0)
    created_at = Column(DateTime(timezone=True), default=func.now())
    updated_at = Column(DateTime(timezone=True), default=func.now(), onupdate=func.now())

class Fragment(Base):
    __tablename__ = 'fragments'
    
    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    meeting_id = Column(Integer, ForeignKey('meetings.id', ondelete='CASCADE'), nullable=False)
    start_time = Column(Float, nullable=False)
    end_time = Column(Float, nullable=False)
    speaker_id = Column(Integer, ForeignKey('speakers.id'), nullable=False)
    text = Column(Text, nullable=False)
    raw_text = Column(Text, nullable=False)
    semantic_cluster = Column(Integer)
    importance_score = Column(Float, default=0.5)
    business_value = Column(String(50))
    technical_terms = Column(JSON, default=[])
    qdrant_id = Column(String(36))
    is_edited = Column(Boolean, default=False)
    created_at = Column(DateTime(timezone=True), default=func.now())
    updated_at = Column(DateTime(timezone=True), default=func.now(), onupdate=func.now())
    
    meeting = relationship("Meeting")
    speaker = relationship("Speaker")

def init_db():
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö"""
    try:
        database_url = os.getenv('DATABASE_URL')
        if not database_url:
            raise ValueError("DATABASE_URL –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –≤ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è")
        
        engine = create_engine(
            database_url,
            pool_size=5,
            max_overflow=10,
            pool_timeout=30,
            pool_recycle=1800,
            connect_args={
                "keepalives": 1,
                "keepalives_idle": 30,
                "keepalives_interval": 10,
                "keepalives_count": 5,
            }
        )
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –≤—Å–µ—Ö —Ç–∞–±–ª–∏—Ü
        Base.metadata.create_all(engine)
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è
        with engine.connect() as connection:
            version = connection.execute(text("SELECT version()")).scalar()
            print(f"‚úÖ Postgres –ø–æ–¥–∫–ª—é—á–µ–Ω —É—Å–ø–µ—à–Ω–æ! –í–µ—Ä—Å–∏—è: {version}")
        
        return engine
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö: {e}")
        raise

def get_db_session(engine=None):
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–µ—Å—Å–∏–∏ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö"""
    if engine is None:
        engine = init_db()
    
    Session = sessionmaker(bind=engine)
    return Session()

***
***
src/legacy/storage/qdrant.py
***
import os
from dotenv import load_dotenv
from qdrant_client import QdrantClient
from qdrant_client.http.models import Distance, VectorParams

load_dotenv()

def init_qdrant_client():
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Qdrant –∫–ª–∏–µ–Ω—Ç–∞"""
    try:
        host = os.getenv('QDRANT_HOST', 'localhost')
        port = int(os.getenv('QDRANT_PORT', '6333'))
        api_key = os.getenv('QDRANT_API_KEY', None)
        
        # –û—Ç–∫–ª—é—á–µ–Ω–∏–µ HTTPS –¥–ª—è –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è
        client = QdrantClient(
            host=host,
            port=port,
            api_key=api_key,
            https=False,
            prefer_grpc=False
        )
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è
        collections = client.get_collections()
        collection_names = [col.name for col in collections.collections]
        print(f"‚úÖ Qdrant –ø–æ–¥–∫–ª—é—á–µ–Ω —É—Å–ø–µ—à–Ω–æ! –°—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –∫–æ–ª–ª–µ–∫—Ü–∏–∏: {collection_names}")
        
        return client
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ Qdrant: {e}")
        raise

def create_collections_if_not_exists(client):
    """–°–æ–∑–¥–∞–Ω–∏–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –∫–æ–ª–ª–µ–∫—Ü–∏–π –µ—Å–ª–∏ –æ–Ω–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É—é—Ç"""
    try:
        # –ö–æ–ª–ª–µ–∫—Ü–∏—è –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤
        if not any(col.name == "semantic_fragments" for col in client.get_collections().collections):
            client.create_collection(
                collection_name="semantic_fragments",
                vectors_config=VectorParams(
                    size=384,  # –†–∞–∑–º–µ—Ä —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ all-MiniLM-L6-v2
                    distance=Distance.COSINE
                )
            )
            print("‚úÖ –ö–æ–ª–ª–µ–∫—Ü–∏—è 'semantic_fragments' —Å–æ–∑–¥–∞–Ω–∞ –≤ Qdrant")
        
        # –ö–æ–ª–ª–µ–∫—Ü–∏—è –¥–ª—è —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤
        if not any(col.name == "technical_terms" for col in client.get_collections().collections):
            client.create_collection(
                collection_name="technical_terms",
                vectors_config=VectorParams(
                    size=384,
                    distance=Distance.COSINE
                )
            )
            print("‚úÖ –ö–æ–ª–ª–µ–∫—Ü–∏—è 'technical_terms' —Å–æ–∑–¥–∞–Ω–∞ –≤ Qdrant")
        
        # –ö–æ–ª–ª–µ–∫—Ü–∏—è –¥–ª—è –±–∏–∑–Ω–µ—Å-–∫–æ–Ω—Ü–µ–ø—Ü–∏–π
        if not any(col.name == "business_concepts" for col in client.get_collections().collections):
            client.create_collection(
                collection_name="business_concepts",
                vectors_config=VectorParams(
                    size=384,
                    distance=Distance.COSINE
                )
            )
            print("‚úÖ –ö–æ–ª–ª–µ–∫—Ü–∏—è 'business_concepts' —Å–æ–∑–¥–∞–Ω–∞ –≤ Qdrant")
            
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –∫–æ–ª–ª–µ–∫—Ü–∏–π –≤ Qdrant: {e}")
        raise

***
***
src/legacy/ai/__init__.py
***

***
***
src/legacy/ai/generator.py
***
"""
–ï–¥–∏–Ω—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –±—ç–∫–µ–Ω–¥–æ–≤
"""
import ollama
import torch
import gc
from pathlib import Path
from src.legacy.config.models import ModelProfile

# –ö—ç—à –¥–ª—è –ª–µ–Ω–∏–≤–æ–π –∑–∞–≥—Ä—É–∑–∫–∏
_OLLAMA_CLIENT = ollama.Client()
_LLAMA_CPP_MODEL = None
_LLAMA_CPP_PROFILE_KEY = None

def _free_gpu_memory():
    """–ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–µ –æ—Å–≤–æ–±–æ–∂–¥–µ–Ω–∏–µ –ø–∞–º—è—Ç–∏"""
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.synchronize()
        gc.collect()

def _generate_ollama(prompt: str, profile: ModelProfile) -> str:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —á–µ—Ä–µ–∑ Ollama API"""
    generation_options = {
        "temperature": profile.params.get("temperature", 0.1),
        "top_p": profile.params.get("top_p", 0.9),
        "repeat_penalty": profile.params.get("repeat_penalty", 1.15),
        "num_predict": profile.params.get("num_predict", 2000),
    }
    generation_options = {k: v for k, v in generation_options.items() if v is not None}
    
    try:
        response = _OLLAMA_CLIENT.chat(
            model=profile.name,
            messages=[{"role": "user", "content": prompt}],
            options=generation_options,
            stream=False
        )
        return response["message"]["content"].strip()
    except Exception as e:
        raise RuntimeError(f"Ollama –æ—à–∏–±–∫–∞ ({profile.name}): {e}")

def _load_llama_cpp_model(profile: ModelProfile):
    """–õ–µ–Ω–∏–≤–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ llama.cpp —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º –∏ –æ—á–∏—Å—Ç–∫–æ–π –ø–∞–º—è—Ç–∏"""
    global _LLAMA_CPP_MODEL, _LLAMA_CPP_PROFILE_KEY
    if _LLAMA_CPP_MODEL is not None and _LLAMA_CPP_PROFILE_KEY == profile.key:
        return _LLAMA_CPP_MODEL
    
    # üî• –û—á–∏—Å—Ç–∫–∞ –ø–µ—Ä–µ–¥ –∑–∞–≥—Ä—É–∑–∫–æ–π –Ω–æ–≤–æ–π –º–æ–¥–µ–ª–∏
    _free_gpu_memory()
    
    model_path = Path(profile.path).resolve()
    if not model_path.exists():
        raise FileNotFoundError(f"–§–∞–π–ª –º–æ–¥–µ–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω: {model_path}")
    
    from llama_cpp import Llama
    load_params = {
        "model_path": str(model_path),
        "n_ctx": profile.params.get("n_ctx", 4096),
        "n_gpu_layers": profile.params.get("n_gpu_layers", 35),
        "n_batch": profile.params.get("n_batch", 512),
        "verbose": profile.params.get("verbose", False),
    }
    print(f"‚è≥ –ó–∞–≥—Ä—É–∑–∫–∞ {profile.key} –≤ GPU (—Å–ª–æ—ë–≤: {load_params['n_gpu_layers']})...")
    _LLAMA_CPP_MODEL = Llama(**load_params)
    _LLAMA_CPP_PROFILE_KEY = profile.key
    print(f"‚úÖ {profile.key} –≥–æ—Ç–æ–≤–∞")
    return _LLAMA_CPP_MODEL

def _format_prompt_for_model(prompt: str, model_key: str) -> str:
    """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–∞ –ø–æ–¥ —Å–ø–µ—Ü–∏—Ñ–∏–∫—É –º–æ–¥–µ–ª–∏ (–±–µ–∑ –¥—É–±–ª–∏—Ä—É—é—â–µ–≥–æ <s>)"""
    if "mistral" in model_key.lower():
        return f"[INST] {prompt} [/INST]"
    elif "phi3" in model_key.lower():
        return f"<|user|>\n{prompt}<|end|>\n<|assistant|>\n"
    else:
        return prompt

def _generate_llama_cpp(prompt: str, profile: ModelProfile) -> str:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —á–µ—Ä–µ–∑ llama-cpp-python"""
    try:
        llm = _load_llama_cpp_model(profile)
        formatted_prompt = _format_prompt_for_model(prompt, profile.key)
        generation_params = {
            "prompt": formatted_prompt,
            "temperature": profile.params.get("temperature", 0.1),
            "top_p": profile.params.get("top_p", 0.9),
            "repeat_penalty": profile.params.get("repeat_penalty", 1.15),
            "max_tokens": profile.params.get("max_tokens", 2048),
            "stop": ["</s>", "<|end|>", "<|user|>", "<|assistant|>", "[INST]", "[/INST]"],
        }
        generation_params = {k: v for k, v in generation_params.items() if v is not None}
        response = llm(**generation_params)
        text = response["choices"][0]["text"].strip()
        for token in ["</s>", "<|end|>", "<|user|>", "<|assistant|>", "[INST]", "[/INST]"]:
            text = text.replace(token, "").strip()
        return text
    except Exception as e:
        raise RuntimeError(f"llama-cpp –æ—à–∏–±–∫–∞ ({profile.key}): {e}")

def generate_text(prompt: str, profile: ModelProfile) -> str:
    """–ï–¥–∏–Ω–∞—è —Ç–æ—á–∫–∞ –≤—Ö–æ–¥–∞ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏"""
    if profile.backend == "ollama":
        return _generate_ollama(prompt, profile)
    elif profile.backend == "llama_cpp":
        return _generate_llama_cpp(prompt, profile)
    else:
        raise ValueError(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –±—ç–∫–µ–Ω–¥: {profile.backend}")
***
***
src/storage/postgres.py
***
import os
from sqlalchemy import create_engine, Column, Integer, String, Float, DateTime, Boolean, Text, JSON, ForeignKey, func
from sqlalchemy import text
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker, relationship
from sqlalchemy.dialects.postgresql import UUID
from datetime import datetime, timezone
import uuid
from dotenv import load_dotenv

load_dotenv()

Base = declarative_base()

class Meeting(Base):
    __tablename__ = 'meetings'
    
    id = Column(Integer, primary_key=True)
    filename = Column(String, unique=True, nullable=False)
    start_time = Column(DateTime(timezone=True), nullable=False, default=func.now())
    duration_sec = Column(Integer, nullable=False)
    audio_hash = Column(String(64), unique=True, nullable=False)
    processed_at = Column(DateTime(timezone=True), default=func.now())
    status = Column(String(20), default='raw')
    quality_score = Column(Float)
    context_tags = Column(JSON, default=[])
    created_at = Column(DateTime(timezone=True), default=func.now())
    updated_at = Column(DateTime(timezone=True), default=func.now(), onupdate=func.now())

class Speaker(Base):
    __tablename__ = 'speakers'
    
    id = Column(Integer, primary_key=True)
    external_id = Column(String(50), unique=True)
    name = Column(String(100))
    role = Column(String(50))
    voice_embedding = Column(JSON)
    confidence_score = Column(Float)
    last_confirmed = Column(DateTime(timezone=True))
    is_verified = Column(Boolean, default=False)
    voice_samples_count = Column(Integer, default=0)
    created_at = Column(DateTime(timezone=True), default=func.now())
    updated_at = Column(DateTime(timezone=True), default=func.now(), onupdate=func.now())

class Fragment(Base):
    __tablename__ = 'fragments'
    
    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    meeting_id = Column(Integer, ForeignKey('meetings.id', ondelete='CASCADE'), nullable=False)
    start_time = Column(Float, nullable=False)
    end_time = Column(Float, nullable=False)
    speaker_id = Column(Integer, ForeignKey('speakers.id'), nullable=False)
    text = Column(Text, nullable=False)
    raw_text = Column(Text, nullable=False)
    semantic_cluster = Column(Integer)
    importance_score = Column(Float, default=0.5)
    business_value = Column(String(50))
    technical_terms = Column(JSON, default=[])
    qdrant_id = Column(String(36))
    is_edited = Column(Boolean, default=False)
    created_at = Column(DateTime(timezone=True), default=func.now())
    updated_at = Column(DateTime(timezone=True), default=func.now(), onupdate=func.now())
    
    meeting = relationship("Meeting")
    speaker = relationship("Speaker")

def init_db():
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö"""
    try:
        database_url = os.getenv('DATABASE_URL')
        if not database_url:
            raise ValueError("DATABASE_URL –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –≤ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è")
        
        engine = create_engine(
            database_url,
            pool_size=5,
            max_overflow=10,
            pool_timeout=30,
            pool_recycle=1800,
            connect_args={
                "keepalives": 1,
                "keepalives_idle": 30,
                "keepalives_interval": 10,
                "keepalives_count": 5,
            }
        )
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –≤—Å–µ—Ö —Ç–∞–±–ª–∏—Ü
        Base.metadata.create_all(engine)
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è
        with engine.connect() as connection:
            version = connection.execute(text("SELECT version()")).scalar()
            print(f"‚úÖ Postgres –ø–æ–¥–∫–ª—é—á–µ–Ω —É—Å–ø–µ—à–Ω–æ! –í–µ—Ä—Å–∏—è: {version}")
        
        return engine
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö: {e}")
        raise

def get_db_session(engine=None):
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–µ—Å—Å–∏–∏ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö"""
    if engine is None:
        engine = init_db()
    
    Session = sessionmaker(bind=engine)
    return Session()

***
***
src/storage/qdrant.py
***
import os
from dotenv import load_dotenv
from qdrant_client import QdrantClient
from qdrant_client.http.models import Distance, VectorParams

load_dotenv()

def init_qdrant_client():
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Qdrant –∫–ª–∏–µ–Ω—Ç–∞"""
    try:
        host = os.getenv('QDRANT_HOST', 'localhost')
        port = int(os.getenv('QDRANT_PORT', '6333'))
        api_key = os.getenv('QDRANT_API_KEY', None)
        
        # –û—Ç–∫–ª—é—á–µ–Ω–∏–µ HTTPS –¥–ª—è –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è
        client = QdrantClient(
            host=host,
            port=port,
            api_key=api_key,
            https=False,
            prefer_grpc=False
        )
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è
        collections = client.get_collections()
        collection_names = [col.name for col in collections.collections]
        print(f"‚úÖ Qdrant –ø–æ–¥–∫–ª—é—á–µ–Ω —É—Å–ø–µ—à–Ω–æ! –°—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –∫–æ–ª–ª–µ–∫—Ü–∏–∏: {collection_names}")
        
        return client
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ Qdrant: {e}")
        raise

def create_collections_if_not_exists(client):
    """–°–æ–∑–¥–∞–Ω–∏–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –∫–æ–ª–ª–µ–∫—Ü–∏–π –µ—Å–ª–∏ –æ–Ω–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É—é—Ç"""
    try:
        # –ö–æ–ª–ª–µ–∫—Ü–∏—è –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤
        if not any(col.name == "semantic_fragments" for col in client.get_collections().collections):
            client.create_collection(
                collection_name="semantic_fragments",
                vectors_config=VectorParams(
                    size=384,  # –†–∞–∑–º–µ—Ä —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ all-MiniLM-L6-v2
                    distance=Distance.COSINE
                )
            )
            print("‚úÖ –ö–æ–ª–ª–µ–∫—Ü–∏—è 'semantic_fragments' —Å–æ–∑–¥–∞–Ω–∞ –≤ Qdrant")
        
        # –ö–æ–ª–ª–µ–∫—Ü–∏—è –¥–ª—è —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤
        if not any(col.name == "technical_terms" for col in client.get_collections().collections):
            client.create_collection(
                collection_name="technical_terms",
                vectors_config=VectorParams(
                    size=384,
                    distance=Distance.COSINE
                )
            )
            print("‚úÖ –ö–æ–ª–ª–µ–∫—Ü–∏—è 'technical_terms' —Å–æ–∑–¥–∞–Ω–∞ –≤ Qdrant")
        
        # –ö–æ–ª–ª–µ–∫—Ü–∏—è –¥–ª—è –±–∏–∑–Ω–µ—Å-–∫–æ–Ω—Ü–µ–ø—Ü–∏–π
        if not any(col.name == "business_concepts" for col in client.get_collections().collections):
            client.create_collection(
                collection_name="business_concepts",
                vectors_config=VectorParams(
                    size=384,
                    distance=Distance.COSINE
                )
            )
            print("‚úÖ –ö–æ–ª–ª–µ–∫—Ü–∏—è 'business_concepts' —Å–æ–∑–¥–∞–Ω–∞ –≤ Qdrant")
            
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –∫–æ–ª–ª–µ–∫—Ü–∏–π –≤ Qdrant: {e}")
        raise

***
***
src/ai/__init__.py
***

***
***
src/ai/generator.py
***
"""
–ï–¥–∏–Ω—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –±—ç–∫–µ–Ω–¥–æ–≤
"""
import ollama
import torch
import gc
from pathlib import Path
from src.config.models import ModelProfile

# –ö—ç—à –¥–ª—è –ª–µ–Ω–∏–≤–æ–π –∑–∞–≥—Ä—É–∑–∫–∏
_OLLAMA_CLIENT = ollama.Client()
_LLAMA_CPP_MODEL = None
_LLAMA_CPP_PROFILE_KEY = None

def _free_gpu_memory():
    """–ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–µ –æ—Å–≤–æ–±–æ–∂–¥–µ–Ω–∏–µ –ø–∞–º—è—Ç–∏"""
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.synchronize()
        gc.collect()

def _generate_ollama(prompt: str, profile: ModelProfile) -> str:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —á–µ—Ä–µ–∑ Ollama API"""
    generation_options = {
        "temperature": profile.params.get("temperature", 0.1),
        "top_p": profile.params.get("top_p", 0.9),
        "repeat_penalty": profile.params.get("repeat_penalty", 1.15),
        "num_predict": profile.params.get("num_predict", 2000),
    }
    generation_options = {k: v for k, v in generation_options.items() if v is not None}
    
    try:
        response = _OLLAMA_CLIENT.chat(
            model=profile.name,
            messages=[{"role": "user", "content": prompt}],
            options=generation_options,
            stream=False
        )
        return response["message"]["content"].strip()
    except Exception as e:
        raise RuntimeError(f"Ollama –æ—à–∏–±–∫–∞ ({profile.name}): {e}")

def _load_llama_cpp_model(profile: ModelProfile):
    """–õ–µ–Ω–∏–≤–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ llama.cpp —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º –∏ –æ—á–∏—Å—Ç–∫–æ–π –ø–∞–º—è—Ç–∏"""
    global _LLAMA_CPP_MODEL, _LLAMA_CPP_PROFILE_KEY
    if _LLAMA_CPP_MODEL is not None and _LLAMA_CPP_PROFILE_KEY == profile.key:
        return _LLAMA_CPP_MODEL
    
    # üî• –û—á–∏—Å—Ç–∫–∞ –ø–µ—Ä–µ–¥ –∑–∞–≥—Ä—É–∑–∫–æ–π –Ω–æ–≤–æ–π –º–æ–¥–µ–ª–∏
    _free_gpu_memory()
    
    model_path = Path(profile.path).resolve()
    if not model_path.exists():
        raise FileNotFoundError(f"–§–∞–π–ª –º–æ–¥–µ–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω: {model_path}")
    
    from llama_cpp import Llama
    load_params = {
        "model_path": str(model_path),
        "n_ctx": profile.params.get("n_ctx", 4096),
        "n_gpu_layers": profile.params.get("n_gpu_layers", 35),
        "n_batch": profile.params.get("n_batch", 512),
        "verbose": profile.params.get("verbose", False),
    }
    print(f"‚è≥ –ó–∞–≥—Ä—É–∑–∫–∞ {profile.key} –≤ GPU (—Å–ª–æ—ë–≤: {load_params['n_gpu_layers']})...")
    _LLAMA_CPP_MODEL = Llama(**load_params)
    _LLAMA_CPP_PROFILE_KEY = profile.key
    print(f"‚úÖ {profile.key} –≥–æ—Ç–æ–≤–∞")
    return _LLAMA_CPP_MODEL

def _format_prompt_for_model(prompt: str, model_key: str) -> str:
    """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–∞ –ø–æ–¥ —Å–ø–µ—Ü–∏—Ñ–∏–∫—É –º–æ–¥–µ–ª–∏ (–±–µ–∑ –¥—É–±–ª–∏—Ä—É—é—â–µ–≥–æ <s>)"""
    if "mistral" in model_key.lower():
        return f"[INST] {prompt} [/INST]"
    elif "phi3" in model_key.lower():
        return f"<|user|>\n{prompt}<|end|>\n<|assistant|>\n"
    else:
        return prompt

def _generate_llama_cpp(prompt: str, profile: ModelProfile) -> str:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —á–µ—Ä–µ–∑ llama-cpp-python"""
    try:
        llm = _load_llama_cpp_model(profile)
        formatted_prompt = _format_prompt_for_model(prompt, profile.key)
        generation_params = {
            "prompt": formatted_prompt,
            "temperature": profile.params.get("temperature", 0.1),
            "top_p": profile.params.get("top_p", 0.9),
            "repeat_penalty": profile.params.get("repeat_penalty", 1.15),
            "max_tokens": profile.params.get("max_tokens", 2048),
            "stop": ["</s>", "<|end|>", "<|user|>", "<|assistant|>", "[INST]", "[/INST]"],
        }
        generation_params = {k: v for k, v in generation_params.items() if v is not None}
        response = llm(**generation_params)
        text = response["choices"][0]["text"].strip()
        for token in ["</s>", "<|end|>", "<|user|>", "<|assistant|>", "[INST]", "[/INST]"]:
            text = text.replace(token, "").strip()
        return text
    except Exception as e:
        raise RuntimeError(f"llama-cpp –æ—à–∏–±–∫–∞ ({profile.key}): {e}")

def generate_text(prompt: str, profile: ModelProfile) -> str:
    """–ï–¥–∏–Ω–∞—è —Ç–æ—á–∫–∞ –≤—Ö–æ–¥–∞ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏"""
    if profile.backend == "ollama":
        return _generate_ollama(prompt, profile)
    elif profile.backend == "llama_cpp":
        return _generate_llama_cpp(prompt, profile)
    else:
        raise ValueError(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –±—ç–∫–µ–Ω–¥: {profile.backend}")
***
