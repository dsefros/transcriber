# =============================================================================
# Конфигурация профилей моделей для анализа технических встреч
# =============================================================================
#
# Структура:
#   default_model: <ключ_профиля>          # Профиль по умолчанию (если не задан в .env)
#   profiles:
#     <ключ_профиля>:
#       backend: ollama | llama_cpp          # Бэкенд генерации
#       name: <имя_в_ollama>                 # Только для backend: ollama
#       path: <путь_к_.gguf>                 # Только для backend: llama_cpp
#       description: "Описание"              # Для списка профилей
#       params:                              # Параметры генерации
#         temperature: 0.1
#         top_p: 0.9
#         repeat_penalty: 1.15
#         # Для Ollama:
#         num_predict: 2000
#         num_ctx: 131072
#         # Для llama_cpp:
#         max_tokens: 2048
#         n_ctx: 4096
#         n_gpu_layers: 25
#         n_batch: 256
#         verbose: false
#
#
# Добавление новой модели:
#   1. Скачайте .gguf в /home/dsefros/models_ai/ (или другую директорию)
#   2. Добавьте блок в секцию `profiles:` ниже
#   3. Установите в .env: ACTIVE_MODEL_PROFILE=<ваш_ключ>
#   4. Перезапустите пайплайн
#
# ===========================================================================


default_model: mistral_8b_q6kl

profiles:
  phi3:medium-128k:
    backend: ollama
    name: phi3:medium-128k
    description: "Основная модель для анализа встреч (128K контекст) в ollama"
    params:
      temperature: 0.1
      top_p: 0.9
      repeat_penalty: 1.15
      num_predict: 2000
      num_ctx: 131072

  llama3:8b-instruct-q5_K_M:
    backend: ollama
    name: llama3:8b-instruct-q5_K_M
    description: "Быстрая модель для чернового анализа"
    params:
      temperature: 0.3
      top_p: 0.85
      repeat_penalty: 1.2
      num_predict: 1500
      num_ctx: 32768

  phi3:mini-128k:
    backend: ollama
    name: phi3:mini-128k
    description: "Самая лёгкая модель для тестов"
    params:
      temperature: 0.2
      top_p: 0.9
      repeat_penalty: 1.15
      num_predict: 1000
      num_ctx: 131072
      
  qwen:7b-chat-v1.5-q5_K_M:
    backend: ollama
    name: qwen:7b-chat-v1.5-q5_K_M
    description: "Быстрая модель для чернового анализа"
    params:
      temperature: 0.2
      top_p: 0.9
      repeat_penalty: 1.15
      num_predict: 1000
      num_ctx: 131072
      
  mistral7b_v02_q4km:
    backend: llama_cpp
    path: /home/dsefros/models_ai/mistral-7b-instruct-v0.2.Q4_K_M.gguf
    description: "Основная модель для анализа встреч в llama_cpp"
    params:
      temperature: 0.15
      top_p: 0.9
      repeat_penalty: 1.2
      max_tokens: 2500
      n_ctx: 16384
      n_gpu_layers: 33
      n_batch: 512
      verbose: true
      
  mistral7b_v03_q6k:
    backend: llama_cpp
    path: /home/dsefros/models_ai/Mistral-7B-Instruct-v0.3-Q6_K.gguf
    description: "Mistral 7B v0.3, Q6_K — выше качество, выше требования к памяти"
    params:
      temperature: 0.15
      top_p: 0.9
      repeat_penalty: 1.2
      max_tokens: 2500
      n_ctx: 16384
      n_gpu_layers: 33
      n_batch: 512
      verbose: true
      
  qwen2_7b_q5km:
    backend: llama_cpp
    path: /home/dsefros/models_ai/qwen2-7b-instruct-q5_k_m.gguf
    description: "Qwen2 7B Instruct, Q5_K_M — баланс скорости и качества"
    params:
      temperature: 0.15
      top_p: 0.9
      repeat_penalty: 1.2
      max_tokens: 2500
      n_ctx: 16384
      n_gpu_layers: 29
      n_batch: 512
      verbose: true
      
  qwen2_7b_q6k:
    backend: llama_cpp
    path: /home/dsefros/models_ai/qwen2-7b-instruct-q6_k.gguf
    description: "Qwen2 7B Instruct, Q6_K — выше качество ТОП!!!"
    params:
      temperature: 0.15
      top_p: 0.9
      repeat_penalty: 1.2
      max_tokens: 2500
      n_ctx: 13384
      n_gpu_layers: 29
      n_batch: 512
      verbose: true
      
  qwen25_7b_q5km:
    backend: llama_cpp
    path: /home/dsefros/models_ai/qwen2.5-7b-instruct-q5_k_m-00001-of-00002.gguf
    description: ""
    params:
      temperature: 0.05
      top_p: 0.9
      repeat_penalty: 1.3
      max_tokens: 2500
      n_ctx: 16384
      n_gpu_layers: 29
      n_batch: 512
      verbose: true

  qwen25_7b_q6k:
    backend: llama_cpp
    path: /home/dsefros/models_ai/qwen2.5-7b-instruct-q6_k-00001-of-00002.gguf
    description: ""
    params:
      temperature: 0.05
      top_p: 0.9
      repeat_penalty: 1.35
      max_tokens: 2500
      n_ctx: 14384
      n_gpu_layers: 29
      n_batch: 512
      verbose: true
      
  mistral3_8b_q5km:
    backend: llama_cpp
    path: /home/dsefros/models_ai/Ministral-3-8B-Instruct-2512-Q5_K_M.gguf
    description: ""
    params:
      temperature: 0.05
      top_p: 0.9
      repeat_penalty: 1.2
      max_tokens: 2500
      n_ctx: 14384
      n_gpu_layers: 29
      n_batch: 512
      verbose: true
      
  mistral_8b_q6kl:
    backend: llama_cpp
    path: /home/dsefros/models_ai/Ministral-8B-Instruct-2410-Q6_K_L.gguf
    description: ""
    params:
      temperature: 0.05
      top_p: 0.9
      repeat_penalty: 1.2
      max_tokens: 2500
      n_ctx: 12384
      n_gpu_layers: 34
      n_batch: 512
      verbose: true
      
  mistral_8b_q5km:
    backend: llama_cpp
    path: /home/dsefros/models_ai/Ministral-8B-Instruct-2410-Q5_K_M.gguf
    description: ""
    params:
      temperature: 0.05
      top_p: 0.9
      repeat_penalty: 1.2
      max_tokens: 2500
      n_ctx: 16384 
      n_gpu_layers: 37
      n_batch: 512
      verbose: false